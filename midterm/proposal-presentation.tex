\documentclass[compress,12pt]{beamer}

\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}

\usetheme{Arguelles}

\title{Multiagent Reinforcement Learning for Traffic Signal Control}
%  \subtitle{Simple, typographic beamer theme}
\event{}
\date{Feb. 12, 2024}
\author{Kevyn Kelso}
\institute{University of Colorado at Colorado Springs}
\email{kkelso@uccs.edu}
\github{KevynKelso}

\begin{document}

\frame[plain]{\titlepage}

\Section{Introduction}

\begin{frame}[bg=arguelles.png]
      \frametitle{Table of Contents}
      \begin{itemize}
      \item Problem Introduction
      \item Problem Formulation within MDP paradigm
      \item Exploration of Existing Methods
      \item Proposed Method
      \item Performance Metrics
      \item Expected Challenges
      \end{itemize}
      % \texttt{\textbackslash frame[plain,bg=demo-background.jpg]\{\textbackslash titlepage\}}
\end{frame}

\begin{frame}[bg=arguelles.png]
      \frametitle{MDP Problem Introduction}
      \begin{itemize}
      \item Signalized intersections contribute to 12-55\% of commute time, with RL methods potentially reducing this by 73\%, guiding this project's aim to enhance RL traffic control strategies \cite{ault2021reinforcement}.
      \item TSC as a POMDP involves partial state visibility and non-stationary dynamics, leading to state aliasing and challenges in centralized approaches due to dimensionality and infrastructure demands\cite{Almeida2022MultiagentRL} \cite{DBLP:journals/corr/abs-2004-04778} \cite{choi1999environment}\cite{choi1999environment} \cite{10.3389/frai.2022.805823}\cite{DBLP:journals/corr/abs-2004-04778} \cite{ault2021reinforcement}.
      \item State Space modeled in \ref{eq:state_space}. \(\rho\) are intersection state variables, \(\Delta_l\) are lane density variables, and \(l \in L\) is No. queued vehicles for each lane.
      \end{itemize}
      % \texttt{\textbackslash frame[plain,bg=demo-background.jpg]\{\textbackslash titlepage\}}

\begin{equation}
s_t = [\rho_1, \rho_2, g, \Delta_1, \ldots, \Delta_L, q_1, \ldots, q_L]
\label{eq:state_space}
\end{equation}

\end{frame}


\Section{MDP}

\begin{frame}[bg=arguelles.png]
      \frametitle{MDP Problem Introduction}
      \begin{itemize}
      \item Agent can select an action to put the intersection into one of four modes \(a_t \in \{a_1, a_2, a_3, a_4\}\).
      \item To change the intersection into a different mode, a mandatory yellow phase $\phi$ precedes the mode change.
      \end{itemize}
      % \texttt{\textbackslash frame[plain,bg=demo-background.jpg]\{\textbackslash titlepage\}}

    \begin{figure}[htbp]
      \centering
      \includegraphics[width=0.8\linewidth]{actions.png}
      \caption{Traffic light modes}
      \label{fig:action_space}
    \end{figure}
\end{frame}

\begin{frame}[bg=arguelles.png]
      \frametitle{Reward Function}
      \begin{itemize}
      \item We use cumulative delay reward function \ref{eq:cumulative_delay} where \(D_t\) is the sum of all vehicles wait time \ref{eq:wait_time_sum}\cite{10.3389/frai.2022.805823}.
      \item \(V_t\) is the set of all vehicles in the simulation.
      \end{itemize}
      % \texttt{\textbackslash frame[plain,bg=demo-background.jpg]\{\textbackslash titlepage\}}

    \begin{equation}
    r_t = D_t - D_{t+1}
    \label{eq:cumulative_delay}
    \end{equation}

    \begin{equation}
    D_t = \sum_{v \in V_t} d_t^v
    \label{eq:wait_time_sum}
    \end{equation}
\end{frame}

\begin{frame}[bg=arguelles.png]
      \frametitle{Environment Simulator}
      \begin{itemize}
      \item Simulated Urban MObility (SUMO) will be used\cite{sumorl}.
      \item Widely accepted and used by the general transportation community\cite{ault2021reinforcement}.
      \item OpenAI PettingZoo API compatible\cite{terry2021pettingzoo}.
      \item Will use 4 x 4 grid scenario.
      \end{itemize}
      % \texttt{\textbackslash frame[plain,bg=demo-background.jpg]\{\textbackslash titlepage\}}

    \begin{figure}[htbp]
      \centering
      \includegraphics[width=0.4\linewidth]{4x4.png}
      \caption{4 x 4 intersection grid\cite{DBLP:journals/corr/abs-2004-04778}}
      \label{fig:intersection_grid}
    \end{figure}

\end{frame}

\begin{frame}[bg=logo.png]
\end{frame}

\Section{Methods}

\begin{frame}[bg=arguelles.png]
      \frametitle{Existing Methods}
      \begin{itemize}
      \item Traditional controllers use either fixed time, max pressure, or greedy\cite{ault2021reinforcement}\cite{Chen_Wei_Xu_Zheng_Yang_Xiong_Xu_Li_2020}.
      \item Independent Deep Q-Learning Networks (IDQN), Independent Proximal Policy Optimization (IPPO), MPLight (variant of DQN)\cite{Chen_Wei_Xu_Zheng_Yang_Xiong_Xu_Li_2020}, Feudal Multiagent Advantage Actor-Critic (FMA2C, MA2C)\cite{DBLP:journals/corr/abs-1903-04527}, State Action Reward State Action Lambda \(SARSA(\lambda)\), TD methods\cite{Reza2023}, Self Adaptive Systems (SAS), and ontology-based RL models\cite{Ghanadbashi2023}.
      \end{itemize}
      % \texttt{\textbackslash frame[plain,bg=demo-background.jpg]\{\textbackslash titlepage\}}

\end{frame}

\begin{frame}[bg=arguelles.png]
      \frametitle{Existing Methods}

    \begin{table}[H]
    \centering
    \small
    \begin{tabular}{lc}
    \hline
    \textbf{Method} & \textbf{Average Cumulative Delay (seconds)} \\ \hline
    Fixed Time      & 90.00\footnotemark[2]  \\
    Max Pressure    & 70.00\footnotemark[2]  \\
    Greedy          & 60.00\footnotemark[2]  \\
    IDQN            & 30.74                  \\
    IPPO            & 36.15                  \\
    MPLight         & 54.58                  \\
    MA2C            & 38.07\footnotemark[1]                  \\
    FMA2C           & 42.26                  \\
    \(SARSA(\lambda)\)           & 18.00\footnotemark[2]                  \\
    Ontology        & 17.00\footnotemark[2]                  \\ \hline
    \end{tabular}
    \caption{Average Delay Across All Scenarios}
    \label{tab:avg_delay}
    \end{table}
    \footnotetext[1]{Data was only collected in one scenario and may not reflect how the model performs in other scenarios.}
    \footnotetext[2]{Data was extracted from graphs.}
\end{frame}

\begin{frame}[bg=arguelles.png]
      \frametitle{Experimental Methodology}
      \begin{itemize}
      \item IDQN is the chosen approach to solve the TSC problem.
      \item IDQN is an off-policy, model-free RL method incorporating the idea of collecting experience tuples as the agent plays episodes\cite{DBLP:journals/corr/MnihKSGAWR13}.
      \item A deep neural network is used as a Q function approximator. Input is the state vector, output is Q values.
      \item Experience tuples used to train the deep neural network. This is known as experience replay.
      \item Actions are selected based on \(\epsilon\)-greedy algorithm where \(\epsilon\) is the probability the agent will choose a random action\cite{Mnih2015}.
      \item Higher values of \(\epsilon\) result in more exploration, while lower values result in more exploitation.
      \end{itemize}
      % \texttt{\textbackslash frame[plain,bg=demo-background.jpg]\{\textbackslash titlepage\}}
\end{frame}

\Section{Conclusion}
\begin{frame}[bg=arguelles.png]
      \frametitle{Challenges Faced}
      \begin{itemize}
      \item Non-stationary dynamics in multiagent 4x4 grid increase complexity compared to single-agent environments.
      \item Fully connected IDQN agents underperformed in 4x4 grid, leading to exclusive use of convolutional IDQN agents.
      \item Utilized research projects for project bootstrap \footnote{https://github.com/LucasAlegre/sumo-rl} \footnote{https://github.com/jault/StateStreetSumo}, requiring debugging and extension for TensorFlow 2.x compatibility and additional feature integration.
      \end{itemize}
      % \texttt{\textbackslash frame[plain,bg=demo-background.jpg]\{\textbackslash titlepage\}}
\end{frame}

\begin{frame}[bg=arguelles.png]
      \frametitle{Timeline}

\begin{table}[H]
\centering
\begin{tabular}{lll}
\hline
\textbf{Done} & \textbf{Date} & \textbf{Milestone}             \\ \hline
\checkmark & 2/19 & 4x4 grid environment setup and working  \\
\checkmark & 2/26 & IDQN agents learning                    \\
\checkmark & 3/4  & Perform experiments and generate data   \\
& 3/18 & Injected uncertainty scenarios          \\
& 3/24 & Binary DQN experiments                  \\
& 4/1 & Double DQN and other misc experiments  \\
& 4/15 & Data compilation and writing            \\ \hline
\end{tabular}
\caption{Project Timeline}
\label{tab:project_timeline}
\end{table}
      % \texttt{\textbackslash frame[plain,bg=demo-background.jpg]\{\textbackslash titlepage\}}
\end{frame}

\begin{frame}[plain,standout]
      \centering
      Any questions? \\
      Thank you for listening!
      \vfill
    \begin{figure}[htbp]
      \centering
      \includegraphics[width=0.4\linewidth]{black_mountain_lion.jpg}
    \end{figure}
\end{frame}

\begin{frame}[allowframebreaks]
        \frametitle{References}
        \bibliographystyle{amsalpha}
        \bibliography{proposal.bib}
\end{frame}

\end{document}
