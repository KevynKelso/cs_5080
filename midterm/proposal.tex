\def\year{2022}\relax
%File: formatting-instructions-latex-2022.tex
%release 2022.1
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai22}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
%\nocopyright
%
% PDF Info Is REQUIRED.
% For /Title, write your title in Mixed Case.
% Don't use accents or commands. Retain the parentheses.
% For /Author, add all authors within the parentheses,
% separated by commas. No accents, special characters
% or commands are allowed.
% Keep the /TemplateVersion tag as is
\pdfinfo{
/Title Multiagent Reinforcement Learning for Traffic Signal Control
/Author Kevyn Kelso
/TemplateVersion (2022.1)
}

\usepackage{amsmath}
\usepackage{amssymb}
% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai22.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Multiagent Reinforcement Learning for Traffic Signal Control}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Kevyn Kelso
}
\affiliations{
    %Afiliations
    University of Colorado-Colorado Springs\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar, \textsuperscript{\rm 2}
    % J. Scott Penberthy, \textsuperscript{\rm 3}
    % George Ferguson,\textsuperscript{\rm 4}
    % Hans Guesgen, \textsuperscript{\rm 5}.
    % Note that the comma should be placed BEFORE the superscript for optimum readability
    1420 Austin Bluffs Pkwy\\
    Colorado Springs, CO 80919\\

    % email address must be in roman text type, not monospace or sans serif
    kkelso@uccs.edu
%
% See more examples next
}

\begin{document}

\maketitle


% - problem definition
% - problem complexity
% - why it's important
%- what benefits to using RL to solve it
% - introduce SUMO environment and benchmark systems

% To talk about:
% ault2021reinforcement
% - https://people.engr.tamu.edu/guni/Papers/NeurIPS-signals.pdf
% - 12â€“55% of commute travel time is due to delays induced by signalized intersections (stopped or approach delay)
% - an efficient traffic signal depends on an accurate estimation of the time it takes to travel between the two.
% - introduce Simulation of Urban Mobility traffic simulator (SUMO) and how its evaluated and calibrated by the general transportation community
% - REinforced Signal COntrol or RESCO is a testbed and benchmark environment for comparison between RL-based signal control algorithms
% - tate of the intersection is usually defined by the set of incoming vehicles (incoming lane, speed, waiting time, queue position) and the current signal (right-of-passage) assignmen
% - Such an approach showed a potential reduction of up to 73% in vehicle delays when compared to fixed-time actuation
% - do research on different reward functions (total delays imposed by the intersection, waiting time at the intersection, traffic pressure)
% - Baseline controllers: Fixed time, max pressure, greedy
% - RL algorithms: IDQN (best), IPPO, MPLight, Extended MPLight
% - decentralized control algorithms may be better than centralized
% - performance was overal worse with more environment information except in the dense city example

% - DBLP:journals/corr/abs-2004-04778
% - file:///home/vyn/Downloads/peerj-cs-575.pdf
% - Partial observability may cause distinct states (in which distinct actions are optimal) to be seen as the same by the traffic signal agent
% - partially obeservable variables change the context of the environment and can include: different traffic patterns in different parts of the day, accidents, road maintenence, weather, actions taken by other agents, and other hazards. This is known as non-stationarity
% - Model-free methods, on the other hand, do not require that the agent have access to information about how the environment works. Instead, they learn an action-value function based only on samples obtained by interacting with the environment. we will be using a model free agent
% - Equations on state formulation. State is commonly limited by cost and availability of sensors in the real world (can model how much improvement can be made if new sensors are installed to make better informed decisions on what infrastructure to purchase?)
% - Typically, simulation time steps correspond to 5-10 seconds of actual traffic simulation because signals do not change on a per second basis.
% - change and keep actions, change comes with a yellow phase of 2 seconds.
% - Multiagent allows for different behaviors between agents and avoids the curse of dimensionality that a centralized training scheme would introduce. However, agents learning and adjusting policies results in changes in the environment dynamics which causes non-stationarity. The best policy for an agent changes as the policies for other agents change.
% - context changes such as an accident increasing the queues of an agent can cause an agent to undergo a readaptation phase to correctly update their policies to deal with the context change which results in periods of catastrophic drops in performance.
% - this paper tested introducing different contexts by changing the rate at which cars entered different sides of a 4 x 4 grid traffic scenario
% - In order to obtain a good performance using fixed policies, it would be necessary to define a policy for each context and to know in advance the exact moment when context changes will occur. Moreover, there may be an arbitrarily large number of such contexts, and the agent, in general, has no way of knowing in advance how many exist.
% - exploration and exploitation: exploration allows agents to adapt to context changes
% -  density attributes are fundamental to better characterize the true state of a traffic intersection. they found that only using queue attributes allowed the policy to converge faster due to the smaller state space, but did not perform as well as queue + density in the end.
% - agents might learn actions that apply to both contexts, this is what we want
% - More state observability helps avoid the negative impacts of non-stationarity.
% - Although they used tabular Q learning methods, they believe the non-stationarity also affects DRL algorithms

% - Ghanadbashi2023
% - file:///home/vyn/Downloads/HandlingUncertaintyinSelf-adaptiveSystems_AnOntology-basedReinforcementLearningModel.pdf
% - however, the current mechanisms do not address rare events in an environmen
% - An ontology is a formal specification of a conceptualization of a domain and the relationships between those concepts. [15],
% - Self Adaptive Systems (SAS)
% - . To capture the underlying uncertainty and variability of dynamic environments, the knowledge model can be formulated as a stochastic dynamic decision problem generally modeled via a POMDP and can be solved using an RL framework. 
% - action masking (primarily filtering out invalid or impossible actions) can be used to improve exploration performance
% - based on the distribution of unanticipated events, they can switch between an ontology based policy and an RL based policy
% - they used an augmented reward signal based on the calculated efficiency of each action
% - they suggest state could be augemented by having detectable vehicles in the system so the traffic controller can know information about the vehicle (emergency vehicle, type of vehicle, length, weight, etc...)
% - they create a table of inference rules (ontology rules) to mask observations (masking irrelavant info can improve performance)
% - RL systems do not perform well handling rare events

%- choi1999environment
% - https://proceedings.neurips.cc/paper_files/paper/1999/file/e8d92f99edd25e2cef48eca48320a1a5-Paper.pdf
% - traditional MDPs assume dynamics of the environment don't change. Classically, elevators are non-stationary
% - non-stationary environments don't come with convergence guarantees that traditional MDPs have
% - must be some regularity in the way the environment changes
% - propose to add "modes" or contexts in addition to the state
% - Basically you have an MDP for each mode, and a way to detect which mode you are in
% - Modes can be modeled in a traffic simulator environment by assigning fixed probabilities for cars waiting at each direction of the intersection, then changing those probabilities in discrete steps for example just switching them
% - agents actions do not affect modes in TSC
% - mode changes are infrequent and number of states are substantially larger than modes

% - hafiz2020deep
% - https://arxiv.org/pdf/2008.04109.pdf
% - shared state and rewards, but agent specific action updating in the experience replay pools = simplicity, faster convergence, and better performance than baseline models
% - non-linear Q functions showed to not perform well due to correlations in the sequence of states, small updates to Q significantly changing the policy
% - experience replay in DQN helps get rid of the correlations by randomizing the experience data used for further training of the network
% - inter-agent communication is complex
% - There are many types of DQN, LDQN, HDQN, WDDQN, DPIQN, DRPIQN
% - binary DQN: each agent corresponds to a possible action the system can take 0 = no action, 1 = take action. AKA CS-DQN
% - simple typically yeilds better performance

% - 10.3389/frai.2022.805823
% - centralized control is intractable for TSC, centralized training, decentralized execution
% - agents typically share the same reward function in cooperative environments, every agent deduces its own contributions based on the team reward
% - having each agent learning independently where each agent treats the others as part of the environment is a bad idea because from each agents perspective, the dynamics appear non-stationary due to other agents learning
% - parameter sharing can be utilized for all independent agents. experience from all agents are trained simultaneously using a single network
% - PPO is highly sensitive to environment and hyperparameters

% - Almeida2022MultiagentRL
% - This relates closely to multiagent systems and to multiagent reinforcement learning, as many problems in traffic management and control are inherently distributed.
% - State space is large and continuous, making it more difficult to properly discretize states, cuasing algorithms to converge more slowly.
% - air pollution, decrease in speed, delays, opportunity costs
% - function approximation comes with some drawbacks including They make the learning harder to comprehend, and generally require that the agents gather a large amount of data to be able to successfully generalize and apply their collected experiences.
% - One of these strategies is ðœ€-greedy, where an action is randomly chosen (exploration) with a probability ðœ€, or, with probability 1-ðœ€, the best known action is chosen, i.e., the one with the highest Q-value so far (exploitation).
% - in many real-world problems, where the control is decentralized, there is no way to avoid a multiagent RL formulation
% - uses a 4x4 grid network of traffic lights
% - experimented with link length between traffic lights
% - raffic signals in our scenario have a minimum and maximum time they must remain green. They are referred to as minGreenTime and maxGreenTime, respectively
% - has great descriptions of state and action spaces
% - they did the same context experiment as the previous paper

% - Reza2023
% - SARSA lambda, sequences of decisions significantly impact the outcome
% - most traffic lights only have inductive sensors reducing performacne for accuracy and efficiency
% - the multi-agent RL techniques are the current state-of-the-art in managing vehicles within road networks
% - Temporal distance (TD), which may be defined as the difference in values at different times, allows an agent to calculate values in a non-deterministic environment
% - also use same reward function as Almeida2022MultiagentRL
% - the signals follow the cycle even if no vehicles are present currently deployed
% - they used a Guassian function to more effectively decay the eligibility traces

% - DBLP:journals/corr/MnihKSGAWR13
% - Deep neural network trained with an experience replay mechanism [13] which randomly samples previous transitions, and thereby smooths the training distribution over many past behaviors
% - n practice, this basic approach is totally impractical, because the action-value function is estimated separately for each sequence, without any generali- sation. Instead, it is common to use a function approximator to estimate the action-value function, Q(s, a; Î¸) â‰ˆ Qâˆ—(s, a).
% - model-free: it solves the reinforcement learning task directly using sam-ples from the emulator E
% - off-policy: it learns about the greedy strategy a = maxa Q(s, a; Î¸), while following a behaviour distribution that ensures adequate exploration of the state spac
% - it was shown that combining model-free reinforcement learning algorithms such as Q- learning with non-linear function approximators [25], or indeed with off-policy learning [1] could cause the Q-network to diverge
% - Since the scale of scores varies greatly from game to game, we fixed all positive rewards to be 1 and all negative rewards to be âˆ’1

% - Mnih2015
% - Reinforcement learning is known to be unstable or even to diverge when a nonlinear function approximator such as a neural network is used to represent the action-value (also known as Q) function - 
% - perhaps using a neural network representing only the valid actions in the output would produce better performance. Network could be selected based on valid actions available (binary DQN?)
% - his manner limits the scale of the error derivatives and makes it easier to use the same learning rate across multiple games. 
% - he fact that there is similar structure in the two-dimensional embeddings corresponding to the DQN representation of states experienced during human play (orange points) and DQN play (blue points) suggests that the representations learned by DQN do indeed generalize to data generated from policies other than its own.
% - plotting the value (V) over time, can see spikes in expected value
% - with replay yields an order of magnitute performance increase

% Begin MIDTERM PREP
% https://arxiv.org/pdf/1912.11023.pdf
% ault2020learning
% - IDQN model with For DQN and the regulatable variants, the minibatch size is set to 32 and replay capacity at 100,000 transitions. The epsilon greedy actionâ€“selection probability in DQN and its variants is reduced from 0.05 to 0 (full exploit) after 20 training episodes resulting in the observed drop in the graphs. The Qâ€“network is a DNN composed of 3 hidden layers with 64 units each, where the first layer is a 2x2 kernel convolutional layer grouping the input for lanes that belong to the same road. The Huber loss function is used in line with the original DQN work. Leaky ReLU activation is used for all layers along with the Adam optimizer with a stepâ€“ size of 0.001, and decay rates Î²1 and Î²2 are set as 0.9 and 0.999 respectively.

% Add section about huber loss

\begin{abstract}
Transportation is a complex problem facing developed societies.
It is riddled with logistical challenges extending beyond pollution and traffic congestion.
The idea of applying reinforcement learning to transportation problems is not new but has great potential for improvement and research in the field.
Contrary to the fixed algorithms currently deployed in most areas \cite{Chaudhuri2021}, reinforcement learning can make decisions in stochastic and uncertain environments characteristic of traffic problems.
Many common reinforcement learning approaches have been applied, showing promise over other approaches, and will be compared below.
This project aims to not only reproduce existing methods but also apply new ideas to improve performance concerning traffic throughput metrics described below.
An integration of various ideas from the literature, combined to study what can be learned from reinforcement learning experimentation with regards to traffic control is the primary purpose.
\end{abstract}

\section{Introduction}
Traffic Signal Control (TSC), a classic control problem affecting all drivers, holds exciting potential for reinforcement learning to address obvious societal problems including air pollution, decreases in speed, delays, and opportunity costs \cite{Almeida2022MultiagentRL}.
Based on travel research conducted in urban areas, 12-55\% of total commute time is caused by signalized intersections (traffic lights) \cite{ault2021reinforcement}.
Moreover, modern reinforcement learning (RL) approaches suggest a potential 73\% reduction in imposed delays compared to traditional approaches currently deployed \cite{ault2021reinforcement}.
Discussions on traffic signal control contain many ideas to increase traffic efficiency in both the fixed algorithm and RL space.
However, many advanced solutions come with expensive caveats. For instance, installing advanced sensors, reworking road paths, or replacing traffic lights with roundabouts \cite{DBLP:journals/corr/abs-2004-04778}.
This project aims to study reinforcement learning approaches to TSC that can be deployed to existing metropolitan areas with minimal cost and infrastructure changes.
In this paper, we will discuss the TSC problem in terms of the Markov Decision Process (MDP) paradigm; and how it is formally described.
Existing methods will then be explored including what is deployed traditionally in most urban environments, and what RL methods could also be used.
Then, our unique solution will be proposed based on information collected comparing a wide variety of methods used previously on the TSC problem \cite{hafiz2020deep} \cite{ault2021reinforcement} \cite{Ghanadbashi2023}.
Results will be discussed regarding the current status of the project showcasing what has been done thus far.
Finally, a discussion of evaluation metrics, challenges overcome, and the timeline of the project will be discussed.

\section{Problem Formulation}
Multiagent approaches to TSC are Partially Observable Markov Decision Processes (POMDPs) with non-stationary dynamics, placing them in one of the most difficult classes of problems for RL to solve \cite{Almeida2022MultiagentRL} \cite{DBLP:journals/corr/abs-2004-04778} \cite{choi1999environment}.
Non-stationary problems do not come with convergence guarantees of traditional MDPs and the partial observability can cause the agent to miss nuance in the state necessary for optimal policies \cite{choi1999environment} \cite{10.3389/frai.2022.805823}.
State aliasing is common in POMDPs where the agent finds two states that are different to be the same, resulting in inappropriate actions.
Additionally, centralized agent approaches suffer from Bellman's curse of dimensionality, require unfeasible infrastructure modifications, and have shown suboptimal performance in simulation \cite{DBLP:journals/corr/abs-2004-04778} \cite{ault2021reinforcement}.

\subsection{State space}
The state space for traffic signal control is a vector of various traffic-related parameters described below.
In the TSC problem, the state space is typically modeled as~\ref{eq:state_space} where each time step \(t\) corresponds to five seconds of actual traffic dynamics, \(\rho_1\) and \(\rho_2\) are binary variables \(\rho_1, \rho_2 \in \{0, 1\}\) indicating the state of the intersection lights, \(g\) indicates if the light has been green for the minimum specified time, \(L\) is the list of all lanes with density \(\Delta_l\) which is the number of vehicles in each lane divided by the capacity of that lane.
\(q_l\) is the number of queued vehicles in each lane \(l \in L\) \cite{Almeida2022MultiagentRL}.

\begin{equation}
s_t = [\rho_1, \rho_2, g, \Delta_1, \ldots, \Delta_L, q_1, \ldots, q_L]
\label{eq:state_space}
\end{equation}

In reality, few intersections contain the sensors needed to make up the state space described in~\ref{eq:state_space}, so it will be interesting to explore how state space restriction affects agent performance.

\subsection{Action space}
The action space is best described in figure \ref{fig:action_space} where the green paths indicate where traffic can flow and the red paths indicate no traffic flow.
The agent can change the intersection into one of four modes \(a_t \in \{a_1, a_2, a_3, a_4\}\) where \(a_1\) = Traffic flows North, South, and right turns.
There are two direct flow modes and two turning modes.
In the direct flow modes North to South and East to West, the vehicles are also permitted to take right turns.
To change the intersection into a different mode, a mandatory yellow phase $\phi$ precedes the mode change.
In most simulation scenarios, (including this project) $\phi$ = 2 seconds.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{actions.png}
  \caption{Traffic light modes \footnote{Alegre, L. N. 2019. SUMO-RL. https://github.com/LucasAlegre/sumo-rl.}}
  \label{fig:action_space}
\end{figure}

\subsection{Reward function}
In most papers, the cumulative delay reward function \ref{eq:cumulative_delay} is used where \(D_t\) is the sum of all vehicles wait time \ref{eq:wait_time_sum}.
Other papers extend this idea to the sum of all vehicle wait times in the system to encourage cooperation between agents \cite{10.3389/frai.2022.805823}.
Furthermore, other sophisticated reward functions such as traffic pressure, augmented rewards based on intersection efficiency, and rewards based on ontology adherence have been explored \cite{ault2021reinforcement} \cite{Ghanadbashi2023}.
In this project, the cumulative delay reward function \ref{eq:cumulative_delay} is used, but further experiments may use other reward functions.
The cumulative delay used in the experiments below is the sum of all vehicles in the system instead of at the intersection level, where \(V_t\) is the set of all vehicles present in the simulation.
The idea behind this is to encourage the agents to work together rather than develop adversarial relationships.
The mean cumulative delay for all vehicles in the system was also tried, yielding acceptable results.

\begin{equation}
r_t = D_t - D_{t+1}
\label{eq:cumulative_delay}
\end{equation}

\begin{equation}
D_t = \sum_{v \in V_t} d_t^v
\label{eq:wait_time_sum}
\end{equation}

\subsection{Environment Simulator}
Accepted and widely used, Simulated Urban MObility (SUMO) environment is used for experimentation\cite{ault2021reinforcement}.
SUMO-RL \footnote{Alegre, L. N. 2019. SUMO-RL. https://github.com/LucasAlegre/sumo-rl.}, an OpenAI PettingZoo API-compatible environment will be used for multiagent simulations \cite{terry2021pettingzoo}.
It is a Python wrapper that controls SUMO via its Traffic Control Interface (TRACI) API.
A 4 x 4 grid structure \ref{fig:intersection_grid} of traffic lights will be used as it is also widely used by other papers \cite{Almeida2022MultiagentRL} and is easily understood.
This requires 16 independent agents.
As a future work item, studying how agents trained on a 4 x 4 grid environment transfer to more complex environments such as the cities of New York or Chicago will be interesting.
Any arbitrary city can be simulated because the map data can be imported into SUMO allowing it to generate an environment fairly easily.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\linewidth]{4x4.png}
  \caption{4 x 4 intersection grid \cite{DBLP:journals/corr/abs-2004-04778}}
  \label{fig:intersection_grid}
\end{figure}

\section{Existing Methods}
% \subsection{Traditional Controllers}
% The traffic controllers currently used in most areas today fall into three categories of algorithms: fixed time, max pressure, and greedy.
% Fixed time controllers will set the traffic signal into one of the four modes for a fixed amount of time, cycling through each mode \cite{ault2021reinforcement}.
% Max pressure is the idea of selecting the traffic flow direction based on subtracting the number of cars after the intersection from the number of cars before the intersection \cite{Chen_Wei_Xu_Zheng_Yang_Xiong_Xu_Li_2020}.
% The greedy algorithm selects the mode that allows the direction with maximum queue length through.

% \subsection{RL Controllers}
Most popular RL methods have been applied to TSC including Independent Deep Q-Learning Networks (IDQN), Independent Proximal Policy Optimization (IPPO), MPLight (variant of DQN) \cite{Chen_Wei_Xu_Zheng_Yang_Xiong_Xu_Li_2020}, Feudal Multiagent Advantage Actor-Critic (FMA2C, MA2C) \cite{DBLP:journals/corr/abs-1903-04527}, State Action Reward State Action Lambda \(SARSA(\lambda)\), TD methods \cite{Reza2023}, Self Adaptive Systems (SAS), and ontology-based RL models \cite{Ghanadbashi2023}.
Based on the results from each of these papers, IDQN appeared to have the most promising performance which will be discussed further below.
Fixed time, max pressure, and greedy algorithms are not RL algorithms but are kept here as a baseline to reference \cite{ault2021reinforcement} \cite{Chen_Wei_Xu_Zheng_Yang_Xiong_Xu_Li_2020}.


\begin{table}[H]
\centering
\begin{tabular}{lc}
\hline
\textbf{Method} & \textbf{Average Cumulative Delay (seconds)} \\ \hline
Fixed Time      & 90.00\footnotemark[2] \\
Max Pressure    & 70.00\footnotemark[2] \\
Greedy          & 60.00\footnotemark[2] \\
IDQN            & 30.74                  \\
IPPO            & 36.15                  \\
MPLight         & 54.58                  \\
MA2C            & 38.07\footnotemark[1]                  \\
FMA2C           & 42.26                  \\
\(SARSA(\lambda)\)           & 18.00\footnotemark[2]                  \\
Ontology        & 17.00\footnotemark[2]                  \\ \hline
\end{tabular}
\caption{Average Delay Across All Scenarios}
\label{tab:avg_delay}
\end{table}
\footnotetext[1]{Data was only collected in one scenario and may not reflect how the model performs in other scenarios.}
\footnotetext[2]{Data was extracted from graphs.}

\section{Experimental Methodology}
Based on performance data collected in other studies \cite{ault2021reinforcement} \cite{Ghanadbashi2023}, IDQN is the chosen approach to solve the TSC problem.
IDQN is an off-policy, model-free RL method that uses a deep neural network (Q-network) to map states to actions.
This method incorporates the idea of collecting experience tuples as the agent plays episodes.
The experience is used to train a deep neural network randomly shuffled to smooth the training data across many past behaviors \cite{DBLP:journals/corr/MnihKSGAWR13}.
The input to the network is the state information, and the output is a Q-value mapped to each action.
% In the past, non-linear Q-value function approximators (such as a deep neural network) were unsuccessful due to correlations in the sequence of states, and small updates to Q significantly changing the policy \cite{hafiz2020deep}.
% Experience replay and other DQN methods were shown to significantly increase the chances of convergence and improve performance by an order of magnitude \cite{Mnih2015}.
Additionally, simplifying the deep neural network to output only binary values of whether to take the action or not yielded better performance than mapping a Q-value to each action. This is called binary DQN or CS-DQN \cite{hafiz2020deep}.
First, IDQN will be used to solve the 4x4 grid TSC environment, then binary DQN will be explored to test if it can improve performance metrics.
An off-policy action selection method known as $\epsilon$-greedy will be used, where $\epsilon$ is the probability the agent will select a random action instead of following the learned policy.
Higher values of $\epsilon$ encourage exploration, while lower values encourage exploitation.
In the experiments below, $epsilon$ decays linearly from 1.0 to 0.1 at a rate of 0.001 per step \cite{Mnih2015}. For performance evaluation, $epsilon = 0$
The experience replay buffer contains 100,000 elements and training starts when the agent has collected at least 64 experience samples \(batch_size = 64\).
% The value of $\epsilon$ does not have to be constant throughout the learning process, and choosing an $\epsilon$ value that decays linearly from 1.0 to 0.1 provided good results \cite{Mnih2015}.

\subsection{Convolutional IDQN}
A convolutional layer was added to the Q-network showing an increase in performance if the state can be represented as an image.
The idea behind this is to create an image in which a 2x2 kernel can convolve gathering more meaningful state information if nearby pixels map to state information belonging to the same road \cite{ault2020learning}.
An example of the state space represented as an image is shown in figure \ref{fig:state_image_rep} where the image displayed on the left is a representation of the traffic shown on the right.
Without the convolutional layer at the start of the network, performance was acceptable on the single-agent environment but did not scale well to the 4x4 grid.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{state_image_rep.png}
  \caption{State Image Representation}
  \label{fig:state_image_rep}
\end{figure}

After the convolutional layer, 2 fully connected layers are added with 64 units each. The table \ref{tab:model_summary} shows a summary of the network.
Huber loss is used as a target for the Q-network to minimize using the Adam optimizer (a variant of stochastic gradient descent) with a learning rate of 0.001.
Essentially, Huber loss is a piecewise function combining the resistance to outliers present in absolute loss $L_{abs}(x) = |x|$ and the fast learning (due to its convex nature) of quadratic loss $L_{quadratic}(x) = x^2$.
Huber loss is represented in equation \ref{eq:huber_loss} with changes to quadratic and absolute loss to make it differentiable.

\begin{equation}
   \label{eq:huber_loss}
    L_{\delta}(x) = \left\{ 
    \begin{array}{ll}
      \frac{1}{2}x^{2}, & \text{for } |x| \leq \delta \\
      \delta\left(|x| - \frac{1}{2}\delta\right), & \text{for } |x| > \delta
    \end{array}
    \right.
\end{equation}

\begin{table}[h]
\centering
\begin{tabular}{lc}
\hline
\textbf{Layer (type)}              & \textbf{Param \#} \\ \hline
\texttt{Conv2D + LeakyReLU}        & 320       \\
\texttt{Flatten}                   & 0         \\
\texttt{Dense + LeakyReLU}         & 81,984    \\
\texttt{Dense + LeakyReLU}         & 4,160     \\
\texttt{Dense}                     & 260       \\ \hline
\textbf{Total trainable params}    & 86,724    \\ \hline
\end{tabular}
\caption{Q-Network Model Summary}
\label{tab:model_summary}
\end{table}

\section{Performance Metrics}
% - RESCO, injected uncertainty, average vehicle wait time
% - Reinforced signal COntrol or RESCO is a testbed and benchmark environment for comparison between RL-based signal control algorithms
REinforced Signal Control (RESCO) is a testbed and benchmark environment to help judge the performance of RL-based algorithms for TSC. It comes built-in with the SUMO project and will simulate traffic in the same way it was simulated in other benchmark papers \cite{ault2021reinforcement}. This can be used to compare this project with what has already been done.
The metrics that are relevant to this project include total wait time for each vehicle, average delay per vehicle, average number of stops, average queue length, and average trip time \cite{Reza2023}.
Studying how the agents respond to injected uncertainty such as an emergency vehicle or increased traffic demands is also an important metric for designing a robust system \cite{DBLP:journals/corr/abs-2004-04778}.
% - total waiting time, the average delay per vehicle, the average number of stops \cite{Reza2023}


% - IDQN model with For DQN and the regulatable variants, the minibatch size is set to 32 and replay capacity at 100,000 transitions.
% - The epsilon greedy actionâ€“-selection probability in DQN and its variants is reduced from 0.05 to 0 (full exploit) after 20 training episodes resulting in the observed drop in the graphs.
% -The Qâ€“network is a DNN composed of 3 hidden layers with 64 units each, where the first layer is a 2x2 kernel convolutional layer grouping the input for lanes that belong to the same road.
% - The Huber loss function is used in line with the original DQN work.
% - Leaky ReLU activation is used for all layers along with the Adam optimizer with a stepâ€“size of 0.001, and decay rates Î²1 and Î²2 are set as 0.9 and 0.999 respectively.

% - Gokcesu2021GeneralizedHL
% - Huber loss paper:
% - resistant to outliers in data
% - learning is fast for quadratic loss to convex nature
% - Huber loss combines resistance to outliers and fast learning
% Huber loss is a piecewise function combining quadratic loss and absolute loss in a differentiable way

\section{Single Intersection Results}
Starting with the simple case, RL methods were applied to a single intersection scenario.
Models that performed well on this task were then selected to be used in the 4x4 grid.

\subsection{Fixed Time Baseline}
To get a baseline to compare the RL approaches, a fixed time agent was implemented using a round robin (RR) approach and cycling the intersection every 50 seconds \cite{Chaudhuri2021}.
Figure \ref{fig:delays_fixed_single} shows the average cumulative delay for this method.
% and figure \ref{fig:total_stopped_fixed_single} shows the total number of stopped vehicles over 5 episodes.
Recall in table \ref{tab:avg_delay} the fixed time algorithm had an average delay of 90 seconds.
However, better performance is expected in the single intersection scenario, and the 90-second number is an average of 5 different scenarios.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{delays_fixed_single.png}
  \caption{Avg. Delay RR Single Signal}
  \label{fig:delays_fixed_single}
\end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.8\linewidth]{total_stopped_fixed_single.png}
%   \caption{Total Stopped RR Single Signal}
%   \label{fig:total_stopped_fixed_single}
% \end{figure}

\subsection{Convolutional DQN}
The convolutional DQN agent was also applied to the single intersection scenario with figure \ref{fig:delays_dqn_single} showing the average cumulative delay,
% \ref{fig:total_stopped_dqn_single} showing total stopped vehicles,
and \ref{fig:loss_dqn_single} showing the Huber loss function over 5 episodes.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{delays_dqn_single.png}
  \caption{Avg. Delay DQN Single Signal}
  \label{fig:delays_dqn_single}
\end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.8\linewidth]{total_stopped_dqn_single.png}
%   \caption{Total Stopped DQN Single Signal}
%   \label{fig:total_stopped_dqn_single}
% \end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{loss_dqn_single.png}
  \caption{Huber Loss DQN Single Signal}
  \label{fig:loss_dqn_single}
\end{figure}

\section{4x4 Grid Results}
For brevity, the fixed time results applied to the 4x4 grid scenario have been left out of this paper.
In summary, the fixed time round-robin approach averages 60 seconds of imposed delays with approximately 100 stopped vehicles in the system at any given time.
16 convolutional IDQN agents were applied to the 4x4 grid, each agent controlling a single traffic signal in the same way as the single intersection experiment.
Figure \ref{fig:delays_dqn_multi} shows average cumulative delay over along with figure \ref{fig:total_stopped_dqn_multi} showing total stopped vehicles.
Figure \ref{fig:loss_dqn_multi} plots the Huber loss for each agent labeled as in the image \ref{fig:intersection_grid} above.
This experiment required many more episodes than the single intersection due to the independent agent's learning affecting other agents.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{../sumo-rl/Data_dqn_multi_2/avg_delay.png}
  \caption{Avg. Delay DQN 4x4}
  \label{fig:delays_dqn_multi}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{../sumo-rl/Data_dqn_multi_2/total_stopped.png}
  \caption{Total Stopped DQN 4x4}
  \label{fig:total_stopped_dqn_multi}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\linewidth]{../sumo-rl/Data_dqn_multi_2/dqn_trainer_loss.png}
  \caption{Huber Loss DQN 4x4}
  \label{fig:loss_dqn_multi}
\end{figure}

\section{Challenges Faced}
As expected, the non-stationary dynamics of the multiagent 4x4 grid environment made it a more difficult problem than the single-agent environment.
More experimentation must be done with different uncertainty scenarios to build a more robust system that is capable of handling non-stationarity.
Unexpectedly, the fully connected IDQN agents did not scale well to the 4x4 grid environment, causing experimentation to switch exclusively to using convolutional IDQN agents.
The research projects \footnote{https://github.com/LucasAlegre/sumo-rl} \footnote{https://github.com/jault/StateStreetSumo} used to bootstrap this project were invaluable but needed debugging and extending to work with Tensorflow 2.x and add the features required for these experiments.
It is important to be a good steward of the open-source RL space, so contributions are currently in the works.
% \section{Expected Challenges}
% The non-stationary nature of multiagent RL problems is known to cause divergence problems due to the constantly changing dynamics.
% This can be somewhat mitigated by applying a scenario that is consistent across training, but the dynamics will still be changing due to the other agents' learning \cite{DBLP:journals/corr/abs-2004-04778}.
% Introducing context switches by changing the scenario, for example, making vehicles flow in waves to simulate rush hour is critical to designing a robust traffic system but will be out of the scope of the initial research goals.

% - injecting uncertainty \cite{DBLP}

\section{Timeline}

\begin{table}[H]
\centering
\begin{tabular}{lll}
\hline
\textbf{Done} & \textbf{Date} & \textbf{Milestone}             \\ \hline
\checkmark & 2/19 & 4x4 grid environment setup and working  \\
\checkmark & 2/26 & IDQN agents learning                    \\
\checkmark & 3/4  & Perform experiments and generate data   \\
& 3/18 & Injected uncertainty scenarios          \\
& 3/24 & Binary DQN experiments                  \\
& 4/1 & Double DQN and other misc experiments  \\
& 4/15 & Data compilation and writing            \\ \hline
\end{tabular}
\caption{Project Timeline}
\label{tab:project_timeline}
\end{table}

\section{Conclusion}
Overall, it was discussed how traffic signal control (TSC) can be formulated in the Markov Decision Process (MDP) paradigm. The state, action, and reward spaces were defined.
Based on research done applying the fixed algorithm and reinforcement learning to TSC, an Independent Deep Q-Learning Network approach was used for experiments, aiming to reduce the average cumulative delay \(D_t\) to 30.74 seconds or lower.
This was achieved using the convolutional IDQN with a cumulative delay of approximately 20 seconds.
Given the partial and non-stationary nature of the TSC problem, its divergence challenges were faced, but overcome. Overall, the remainder of the project will be completed following the Timeline table \ref{tab:project_timeline}.

% \appendix
% \section{Reference Examples}
% \label{sec:reference_examples}

%- \nobibliography*
\bibliography{proposal}
% - \bibentry{em:86}.

% Use \bibliography{yourbibfile} instead or the References section will not appear in your paper

\end{document}
