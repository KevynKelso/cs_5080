\def\year{2022}\relax
%File: formatting-instructions-latex-2022.tex
%release 2022.1
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai22}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
%\nocopyright
%
% PDF Info Is REQUIRED.
% For /Title, write your title in Mixed Case.
% Don't use accents or commands. Retain the parentheses.
% For /Author, add all authors within the parentheses,
% separated by commas. No accents, special characters
% or commands are allowed.
% Keep the /TemplateVersion tag as is
\pdfinfo{
/Title Multiagent Reinforcement Learning for Traffic Signal Control
/Author Kevyn Kelso
/TemplateVersion (2022.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai22.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Multiagent Reinforcement Learning for Traffic Signal Control}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Kevyn Kelso
}
\affiliations{
    %Afiliations
    University of Colorado-Colorado Springs\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar, \textsuperscript{\rm 2}
    % J. Scott Penberthy, \textsuperscript{\rm 3}
    % George Ferguson,\textsuperscript{\rm 4}
    % Hans Guesgen, \textsuperscript{\rm 5}.
    % Note that the comma should be placed BEFORE the superscript for optimum readability
    1420 Austin Bluffs Pkwy\\
    Colorado Springs, CO 80919\\

    % email address must be in roman text type, not monospace or sans serif
    kkelso@uccs.edu
%
% See more examples next
}

\begin{document}

\maketitle

\begin{abstract}
\end{abstract}


% - problem definition
% - problem complexity
% - why it's important
%- what benefits to using RL to solve it
% - introduce SUMO environment and benchmark systems

% To talk about:
% ault2021reinforcement
% - https://people.engr.tamu.edu/guni/Papers/NeurIPS-signals.pdf
% - 12–55% of commute travel time is due to delays induced by signalized intersections (stopped or approach delay)
% - an efficient traffic signal depends on an accurate estimation of the time it takes to travel between the two.
% - introduce Simulation of Urban Mobility traffic simulator (SUMO) and how its evaluated and calibrated by the general transportation community
% - REinforced Signal COntrol or RESCO is a testbed and benchmark environment for comparison between RL-based signal control algorithms
% - tate of the intersection is usually defined by the set of incoming vehicles (incoming lane, speed, waiting time, queue position) and the current signal (right-of-passage) assignmen
% - Such an approach showed a potential reduction of up to 73% in vehicle delays when compared to fixed-time actuation
% - do research on different reward functions (total delays imposed by the intersection, waiting time at the intersection, traffic pressure)
% - Baseline controllers: Fixed time, max pressure, greedy
% - RL algorithms: IDQN (best), IPPO, MPLight, Extended MPLight
% - decentralized control algorithms may be better than centralized
% - performance was overal worse with more environment information except in the dense city example

% - DBLP:journals/corr/abs-2004-04778
% - file:///home/vyn/Downloads/peerj-cs-575.pdf
% - Partial observability may cause distinct states (in which distinct actions are optimal) to be seen as the same by the traffic signal agent
% - partially obeservable variables change the context of the environment and can include: different traffic patterns in different parts of the day, accidents, road maintenence, weather, actions taken by other agents, and other hazards. This is known as non-stationarity
% - Model-free methods, on the other hand, do not require that the agent have access to information about how the environment works. Instead, they learn an action-value function based only on samples obtained by interacting with the environment. we will be using a model free agent
% - Equations on state formulation. State is commonly limited by cost and availability of sensors in the real world (can model how much improvement can be made if new sensors are installed to make better informed decisions on what infrastructure to purchase?)
% - Typically, simulation time steps correspond to 5-10 seconds of actual traffic simulation because signals do not change on a per second basis.
% - change and keep actions, change comes with a yellow phase of 2 seconds.
% - Multiagent allows for different behaviors between agents and avoids the curse of dimensionality that a centralized training scheme would introduce. However, agents learning and adjusting policies results in changes in the environment dynamics which causes non-stationarity. The best policy for an agent changes as the policies for other agents change.
% - context changes such as an accident increasing the queues of an agent can cause an agent to undergo a readaptation phase to correctly update their policies to deal with the context change which results in periods of catastrophic drops in performance.
% - this paper tested introducing different contexts by changing the rate at which cars entered different sides of a 4 x 4 grid traffic scenario
% - In order to obtain a good performance using fixed policies, it would be necessary to define a policy for each context and to know in advance the exact moment when context changes will occur. Moreover, there may be an arbitrarily large number of such contexts, and the agent, in general, has no way of knowing in advance how many exist.
% - exploration and exploitation: exploration allows agents to adapt to context changes
% -  density attributes are fundamental to better characterize the true state of a traffic intersection. they found that only using queue attributes allowed the policy to converge faster due to the smaller state space, but did not perform as well as queue + density in the end.
% - agents might learn actions that apply to both contexts, this is what we want
% - More state observability helps avoid the negative impacts of non-stationarity.
% - Although they used tabular Q learning methods, they believe the non-stationarity also affects DRL algorithms

% - Ghanadbashi2023
% - file:///home/vyn/Downloads/HandlingUncertaintyinSelf-adaptiveSystems_AnOntology-basedReinforcementLearningModel.pdf
% - however, the current mechanisms do not address rare events in an environmen
% - An ontology is a formal specification of a conceptualization of a domain and the relationships between those concepts. [15],
% - Self Adaptive Systems (SAS)
% - . To capture the underlying uncertainty and variability of dynamic environments, the knowledge model can be formulated as a stochastic dynamic decision problem generally modeled via a POMDP and can be solved using an RL framework. 
% - action masking (primarily filtering out invalid or impossible actions) can be used to improve exploration performance
% - based on the distribution of unanticipated events, they can switch between an ontology based policy and an RL based policy
% - they used an augmented reward signal based on the calculated efficiency of each action
% - they suggest state could be augemented by having detectable vehicles in the system so the traffic controller can know information about the vehicle (emergency vehicle, type of vehicle, length, weight, etc...)
% - they create a table of inference rules (ontology rules) to mask observations (masking irrelavant info can improve performance)
% - RL systems do not perform well handling rare events

%- choi1999environment
% - https://proceedings.neurips.cc/paper_files/paper/1999/file/e8d92f99edd25e2cef48eca48320a1a5-Paper.pdf
% - traditional MDPs assume dynamics of the environment don't change. Classically, elevators are non-stationary
% - non-stationary environments don't come with convergeance guarantees that traditional MDPs have
% - must be some regularity in the way the environment changes
% - propose to add "modes" or contexts in addition to the state
% - Basically you have an MDP for each mode, and a way to detect which mode you are in
% - Modes can be modeled in a traffic simulator environment by assigning fixed probabilities for cars waiting at each direction of the intersection, then changing those probabilities in discrete steps for example just switching them
% - agents actions do not affect modes in TSC
% - mode changes are infrequent and number of states are substantially larger than modes

% - hafiz2020deep
% - https://arxiv.org/pdf/2008.04109.pdf
% - shared state and rewards, but agent specific action updating in the experience replay pools = simplicity, faster convergeance, and better performance than baseline models
% - non-linear Q functions showed to not perform well due to correlations in the sequence of states, small updates to Q significantly changing the policy
% - experience replay in DQN helps get rid of the correlations by randomizing the experience data used for further training of the network
% - inter-agent communication is complex
% - There are many types of DQN, LDQN, HDQN, WDDQN, DPIQN, DRPIQN
% - binary DQN: each agent corresponds to a possible action the system can take 0 = no action, 1 = take action. AKA CS-DQN
% - simple typically yeilds better performance

% - 10.3389/frai.2022.805823
% - centralized control is intractable for TSC, centralized training, decentralized execution
% - agents typically share the same reward function in cooperative environments, every agent deduces its own contributions based on the team reward
% - having each agent learning independently where each agent treats the others as part of the environment is a bad idea because from each agents perspective, the dynamics appear non-stationary due to other agents learning
% - parameter sharing can be utilized for all independent agents. experience from all agents are trained simultaneously using a single network
% - PPO is highly sensitive to environment and hyperparameters

% - Almeida2022MultiagentRL
% - This relates closely to multiagent systems and to multiagent reinforcement learning, as many problems in traffic management and control are inherently distributed.
% - State space is large and continuous, making it more difficult to properly discretize states, cuasing algorithms to converge more slowly.
% - air pollution, decrease in speed, delays, opportunity costs
% - function approximation comes with some drawbacks including They make the learning harder to comprehend, and generally require that the agents gather a large amount of data to be able to successfully generalize and apply their collected experiences.
% - One of these strategies is 𝜀-greedy, where an action is randomly chosen (exploration) with a probability 𝜀, or, with probability 1-𝜀, the best known action is chosen, i.e., the one with the highest Q-value so far (exploitation).
% - in many real-world problems, where the control is decentralized, there is no way to avoid a multiagent RL formulation
% - uses a 4x4 grid network of traffic lights
% - experimented with link length between traffic lights
% - raffic signals in our scenario have a minimum and maximum time they must remain green. They are referred to as minGreenTime and maxGreenTime, respectively
% - has great descriptions of state and action spaces
% - they did the same context experiment as the previous paper

% - Reza2023
% - SARSA lambda, sequences of decisions significantly impact the outcome
% - most traffic lights only have inductive sensors reducing performacne for accuracy and efficiency
% - the multi-agent RL techniques are the current state-of-the-art in managing vehicles within road networks
% - Temporal distance (TD), which may be defined as the difference in values at different times, allows an agent to calculate values in a non-deterministic environment
% - also use same reward function as Almeida2022MultiagentRL
% - the signals follow the cycle even if no vehicles are present currently deployed
% - they used a Guassian function to more effectively decay the eligibility traces

% - DBLP:journals/corr/MnihKSGAWR13
% - Deep neural network trained with an experience replay mechanism [13] which randomly samples previous transitions, and thereby smooths the training distribution over many past behaviors
% - n practice, this basic approach is totally impractical, because the action-value function is estimated separately for each sequence, without any generali- sation. Instead, it is common to use a function approximator to estimate the action-value function, Q(s, a; θ) ≈ Q∗(s, a).
% - model-free: it solves the reinforcement learning task directly using sam-ples from the emulator E
% - off-policy: it learns about the greedy strategy a = maxa Q(s, a; θ), while following a behaviour distribution that ensures adequate exploration of the state spac
% - it was shown that combining model-free reinforcement learning algorithms such as Q- learning with non-linear function approximators [25], or indeed with off-policy learning [1] could cause the Q-network to diverge
% - Since the scale of scores varies greatly from game to game, we fixed all positive rewards to be 1 and all negative rewards to be −1

% - Mnih2015
% - Reinforcement learning is known to be unstable or even to diverge when a nonlinear function approximator such as a neural network is used to represent the action-value (also known as Q) function - 
% - perhaps using a neural network representing only the valid actions in the output would produce better performance. Network could be selected based on valid actions available (binary DQN?)
% - his manner limits the scale of the error derivatives and makes it easier to use the same learning rate across multiple games. 
% - he fact that there is similar structure in the two-dimensional embeddings corresponding to the DQN representation of states experienced during human play (orange points) and DQN play (blue points) suggests that the representations learned by DQN do indeed generalize to data generated from policies other than its own.
% - plotting the value (V) over time, can see spikes in expected value
% - with replay yields an order of magnitute performance increase

\section{Introduction}
Traffic Signal Control (TSC) is a problem which reinforcement learning can be applied, aiding in obvious societal problems: air pollution, decrease in speed, delays, and opportunity costs \cite{Almeida2022MultiagentRL}.
Based on travel research done on urban areas, 12-55\% of total commute time is caused by signalized intersections (traffic lights) \cite{ault2021reinforcement}.
Moreover, modern reinforcement learning (RL) approaches suggest a potential 73\% reduction in that time compared to traditional approaches currently deployed \cite{ault2021reinforcement}.
The field of traffic signal control contains many ideas to increase traffic efficiency in both the fixed algorithm and RL space.
However, many advanced solutions come with expensive caveats. For instance, installing advanced sensors, reworking road paths, or replacing traffic lights with roundabouts \cite{DBLP:journals/corr/abs-2004-04778}.
This project is aimed to study reinforcement learning approaches to TSC that can be deployed to existing metropolitan areas with minimal cost and infrastructure changes.
Studying how currently deployed traffic detection infrastructure  is contributing to the state space \(S\) of a Partially Observable Markov Decision Process (POMDP) to make informed future infrastructure decisions is also a goal.
In this paper, we will discuss the TSC problem in terms of the Markov Decision Process (MDP) paridigm; how it is formally described.
Existing methods will then be explored including what is deployed tradditionally in most urban environments, and what RL methods could also be used.
Then, our unique solution will be proposed based on information collected comparing a wide variety of methods used previously on the TSC problem \cite{hafiz2020deep} \cite{ault2021reinforcement} \cite{Ghanadbashi2023}.
Finally, a discussion of evaluation metrics, expected challenges, and timeline of the project will be discussed.

\section{Problem Formulation}
Multiagent approaches to TSC are POMDPs with non-stationary dynamics, placing them in one of the most difficult classes of problems for RL to solve \cite{Almeida2022MultiagentRL} \cite{DBLP:journals/corr/abs-2004-04778} \cite{choi1999environment}.
Non-stationary problems do not come with convergeance guarantees of traditional MDPs and the partial observability can cause an aliasing of states causing the agent to miss nuance necessary for optimal policies \cite{choi1999environment} \cite{10.3389/frai.2022.805823}.
Additionally, centralized agent approaches suffer from Bellman's curse of dimensionality, require unfeasable infrastructure modifications, and have shown suboptimal performance in simulation \cite{DBLP:journals/corr/abs-2004-04778} \cite{ault2021reinforcement}.

\subsection{State space}
In the TSC problem, the state space is typically modeled as~\ref{eq:state_space} where each time step \(t\) corresponds to five seconds of actual traffic dynamics, \(\rho_1\) and \(\rho_2\) are binary variables \(\rho_1, \rho_2 \in {0, 1}\) indicating the state of the intersection lights, \(g\) indicates if the light has been green for the minumum specified time, \(L\) is the list of all lanes with density \(\Delta_l\) which is the number of vehicles in each lane divided by the capacity of that lane.
\(q_l\) is the number of queued vehicles in each lane \(l \in L\) \cite{Almeida2022MultiagentRL}.

\begin{equation}
s_t = [\rho_1, \rho_2, g, \Delta_1, \ldots, \Delta_L, q_1, \ldots, q_L]
\label{eq:state_space}
\end{equation}

In reality, few intersections contain the sensors needed to make up the state space described in~\ref{eq:state_space}, so it will be interesting to explore how state space restriction affects agent performance.

\subsection{Action space}
The action space is best described in the \ref{fig:action_space} figure below where the green paths indicate where traffic can flow and the red paths are not allowed. The agent can change the intersection into one of four modes \(a_t \in {a_1, a_2, a_3, a_4}\).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{actions.png}
  \caption{Traffic light modes}
  \label{fig:action_space}
\end{figure}

\subsection{Reward function}
In most papers, the cumulative delay reward function \ref{eq:cumulative_delay} is used where \(D_t\) is the sum of all vehicle's wait time \ref{eq:wait_time_sum}.
Other papers extend this idea to the sum of all vehicle wait time in the system to encourage cooperation between agents \cite{10.3389/frai.2022.805823} or more sophisticated reward functions like total delays imposed by the intersection, traffic pressure, or augmented rewards based on intersection efficiency and ontology adherance \cite{ault2021reinforcement} \cite{Ghanadbashi2023}.

\begin{equation}
r_t = D_t - D_{t+1}
\label{eq:cumulative_delay}
\end{equation}

\begin{equation}
D_t = \sum_{v \in V_t} d_t^v
\label{eq:wait_time_sum}
\end{equation}

\subsection{Environment Simulator}
The Simulated Urban MObility (SUMO) environment will be used for experimentation because it has been evaluated and calibrated by the general transportation community \cite{ault2021reinforcement} \cite{sumorl} and is widely used in the literature.
A 4 x 4 grid structure \ref{fig:intersection_grid} of traffic lights will be used as it is also widely used by other papers \cite{Almeida2022MultiagentRL}. This will necessitate 16 independent agents.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\linewidth]{4x4.png}
  \caption{4 x 4 intersection grid}
  \label{fig:intersection_grid}
\end{figure}

\section{Existing Methods}
% - introduce cost of more sensors problem
\subsection{Traditional Controllers}
% - fixed time, max pressure, greedy
\subsection{RL Controllers}
% - performance?
% - Rl methods, IDQN, IPPO, MPLight, Extended MPLight
% Self adaptive systems (SAS), ontology + RL agent

\section{Proposed Method}
Based on performance data collected in other studies \cite{ault2021reinforcement} \cite{Ghanadbashi2023},
IDQN is the chosen approach to solve the TSC problem. Other approaches as well as more advanced
simulation methods such as injected uncertainty \cite{DBLP} are also being considered and may be used
to increase performance of initial IDQN testing.

\section{Performance Metrics}
% - RESCO, injected uncertainty, average vehicle wait time
% - total waiting time, average delay per vehicle, average number of stops \cite{Reza2023}

\section{Expected Challenges}

\section{Timeline}

\section{Conclusion}

% \appendix
% \section{Reference Examples}
% \label{sec:reference_examples}

%- \nobibliography*
\bibliography{proposal}
% - \bibentry{em:86}.

% Use \bibliography{yourbibfile} instead or the References section will not appear in your paper

\end{document}
