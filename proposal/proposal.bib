@inproceedings{ault2021reinforcement,
  title={Reinforcement Learning Benchmarks for Traffic Signal Control},
  author={James Ault and Guni Sharon},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
  year={2021},
  url={https://openreview.net/forum?id=LqRSh6V0vR}
}

}@article{Ghanadbashi2023,
  author    = {Saeedeh Ghanadbashi and Zahra Safavifar and Farshad Taebi and Fatemeh Golpayegani},
  title     = {Handling uncertainty in self-adaptive systems: an ontology-based reinforcement learning model},
  journal   = {Journal of Reliable Intelligent Environments},
  year      = {2023},
  date      = {2023/01/06},
  volume    = {},
  number    = {},
  pages     = {},
  doi       = {10.1007/s40860-022-00198-x},
  url       = {https://doi.org/10.1007/s40860-022-00198-x},
  issn      = {2199-4676},
  abstract  = {Ubiquitous and pervasive systems interact with each other and perform actions favoring the emergence of a global desired behavior. ... The results show that the OnCertain model can improve the RL-based systems’ observation and, consequently, their performance in such environments.}
}

@article{DBLP:journals/corr/abs-2004-04778,
  author       = {Lucas Nunes Alegre and
                  Ana L. C. Bazzan and
                  Bruno C. da Silva},
  title        = {Quantifying the Impact of Non-Stationarity in Reinforcement Learning-Based
                  Traffic Signal Control},
  journal      = {CoRR},
  volume       = {abs/2004.04778},
  year         = {2020},
  url          = {https://arxiv.org/abs/2004.04778},
  eprinttype    = {arXiv},
  eprint       = {2004.04778},
  timestamp    = {Tue, 14 Apr 2020 16:40:34 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2004-04778.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{choi1999environment,
  title={An environment model for nonstationary reinforcement learning},
  author={Choi, Samuel and Yeung, Dit-Yan and Zhang, Nevin},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

@misc{hafiz2020deep,
      title={Deep Q-Network Based Multi-agent Reinforcement Learning with Binary Action Agents}, 
      author={Abdul Mueed Hafiz and Ghulam Mohiuddin Bhat},
      year={2020},
      eprint={2008.04109},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@ARTICLE{10.3389/frai.2022.805823,
AUTHOR={Lee, Ken Ming and Ganapathi Subramanian, Sriram and Crowley, Mark},
TITLE={Investigation of independent reinforcement learning algorithms in multi-agent environments},
JOURNAL={Frontiers in Artificial Intelligence},
VOLUME={5},
YEAR={2022},
URL={https://www.frontiersin.org/articles/10.3389/frai.2022.805823},
DOI={10.3389/frai.2022.805823},
ISSN={2624-8212},
ABSTRACT={Independent reinforcement learning algorithms have no theoretical guarantees for finding the best policy in multi-agent settings. However, in practice, prior works have reported good performance with independent algorithms in some domains and bad performance in others. Moreover, a comprehensive study of the strengths and weaknesses of independent algorithms is lacking in the literature. In this paper, we carry out an empirical comparison of the performance of independent algorithms on seven PettingZoo environments that span the three main categories of multi-agent environments, i.e., cooperative, competitive, and mixed. For the cooperative setting, we show that independent algorithms can perform on par with multi-agent algorithms in fully-observable environments, while adding recurrence improves the learning of independent algorithms in partially-observable environments. In the competitive setting, independent algorithms can perform on par or better than multi-agent algorithms, even in more challenging environments. We also show that agents trained via independent algorithms learn to perform well individually, but fail to learn to cooperate with allies and compete with enemies in mixed environments.}
}

@inproceedings{Almeida2022MultiagentRL,
  title={Multiagent Reinforcement Learning for Traffic Signal Control: a k-Nearest Neighbors Based Approach},
  author={Vicente Nejar de Almeida and Ana L{\'u}cia Cetertich Bazzan and Monireh Abdoos},
  booktitle={ATT@IJCAI},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:251258226}
}

@article{Reza2023,
  title={A citywide TD-learning based intelligent traffic signal control for autonomous vehicles: Performance evaluation using SUMO},
  author={Reza, Selim and Ferreira, Marta Campos and Machado, J. J. M. and Tavares, João Manuel R. S.},
  journal={Expert Systems},
  year={2023},
  doi={10.1111/exsy.13301},
  url={https://doi.org/10.1111/exsy.13301},
  note={First published: 05 April 2023}
}

@article{DBLP:journals/corr/MnihKSGAWR13,
  author       = {Volodymyr Mnih and
                  Koray Kavukcuoglu and
                  David Silver and
                  Alex Graves and
                  Ioannis Antonoglou and
                  Daan Wierstra and
                  Martin A. Riedmiller},
  title        = {Playing Atari with Deep Reinforcement Learning},
  journal      = {CoRR},
  volume       = {abs/1312.5602},
  year         = {2013},
  url          = {http://arxiv.org/abs/1312.5602},
  eprinttype    = {arXiv},
  eprint       = {1312.5602},
  timestamp    = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/MnihKSGAWR13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Mnih2015,
  author    = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Andrei A. Rusu and Joel Veness and Marc G. Bellemare and Alex Graves and Martin Riedmiller and Andreas K. Fidjeland and Georg Ostrovski and Stig Petersen and Charles Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and Dharshan Kumaran and Daan Wierstra and Shane Legg and Demis Hassabis},
  title     = {Human-level control through deep reinforcement learning},
  journal   = {Nature},
  volume    = {518},
  number    = {7540},
  pages     = {529--533},
  year      = {2015},
  doi       = {10.1038/nature14236},
  url       = {https://doi.org/10.1038/nature14236},
  abstract  = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
  issn      = {1476-4687},
}

@article{Chen_Wei_Xu_Zheng_Yang_Xiong_Xu_Li_2020, title={Toward A Thousand Lights: Decentralized Deep Reinforcement Learning for Large-Scale Traffic Signal Control}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/5744}, DOI={10.1609/aaai.v34i04.5744}, abstractNote={&lt;p&gt;Traffic congestion plagues cities around the world. Recent years have witnessed an unprecedented trend in applying reinforcement learning for traffic signal control. However, the primary challenge is to control and coordinate traffic lights in large-scale urban networks. No one has ever tested RL models on a network of more than a thousand traffic lights. In this paper, we tackle the problem of multi-intersection traffic signal control, especially for large-scale networks, based on RL techniques and transportation theories. This problem is quite difficult because there are challenges such as scalability, signal coordination, data feasibility, etc. To address these challenges, we (1) design our RL agents utilizing ‘pressure’ concept to achieve signal coordination in region-level; (2) show that implicit coordination could be achieved by individual control agents with well-crafted reward design thus reducing the dimensionality; and (3) conduct extensive experiments on multiple scenarios, including a real-world scenario with 2510 traffic lights in Manhattan, New York City &lt;sup&gt;1&lt;/sup&gt; &lt;sup&gt;2&lt;/sup&gt;.&lt;/p&gt;}, number={04}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Chen, Chacha and Wei, Hua and Xu, Nan and Zheng, Guanjie and Yang, Ming and Xiong, Yuanhao and Xu, Kai and Li, Zhenhui}, year={2020}, month={Apr.}, pages={3414-3421} }

@misc{sumorl,
    author = {Lucas N. Alegre},
    title = {{SUMO-RL}},
    year = {2019},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/LucasAlegre/sumo-rl}},
}
