\def\year{2022}\relax
%File: formatting-instructions-latex-2022.tex
%release 2022.1
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai22}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{amsmath}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
% \usepackage{algorithm}
% \usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
%\nocopyright
%
% PDF Info Is REQUIRED.
% For /Title, write your title in Mixed Case.
% Don't use accents or commands. Retain the parentheses.
% For /Author, add all authors within the parentheses,
% separated by commas. No accents, special characters
% or commands are allowed.
% Keep the /TemplateVersion tag as is
\pdfinfo{
/Title Homework 1
/Author Kevyn Kelso
/TemplateVersion (2022.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai22.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Homework 1}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Kevyn Kelso
}
\affiliations{
    %Afiliations
    University of Colorado-Colorado Springs\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar, \textsuperscript{\rm 2}
    % J. Scott Penberthy, \textsuperscript{\rm 3}
    % George Ferguson,\textsuperscript{\rm 4}
    % Hans Guesgen, \textsuperscript{\rm 5}.
    % Note that the comma should be placed BEFORE the superscript for optimum readability
    1420 Austin Bluffs Pkwy\\
    Colorado Springs, CO 80919\\

    % email address must be in roman text type, not monospace or sans serif
    kkelso@uccs.edu
%
% See more examples next
}

\begin{document}

\maketitle

\begin{abstract}
test
\end{abstract}

\section{Questions}
\begin{enumerate}
    \item How would you represent the various components that are required to program this reinforcement learning agent? Discuss each component and how you implement it. Explain how you model the barriers, and how you make sure the agent does not fall off the edges.
	\begin{itemize}
	    \item I plan to follow the model given in \cite{Morales2020GrokkingDL} where the dynamics are represented as a nested dictionary in Python. First, the outer dictionary keys will include all the states of the grid environment. Then, an inner dictionary will be the value for each state element containing a key for each action, and a value with a list of all transitions for the state-action pair.
	    \item The list will contain: the probability of that transition occuring given the state and action (in this case the probabilities are always 1 due to the non-stochastic nature of the problem), the next state (represented as an integer), the reward (all 0 except for the goal state), and a flag to tell whether the next state is terminal.
	    \item The barriers will be a property of the dynamics dictionary. The actions that would lead the agent to a barrier state will not be included in the dynamics, making it impossible to transition to those barrier states.
	    \item The same idea is applied to the edges.
	    \item Ideally, these dynamics will be automatically generated based on a given grid environment.
	\end{itemize}
    \item You would initialize with a random policy where every action that is possible in a state is equally probable. What does this policy look like?
	\begin{itemize}
	    \item This policy is best viewed in graphical form \ref{fig:random_policy} where arrows pointing left, right, down, and up indicate which action the agent will take in that state.
	    \item Note, this is a snapshot of the random policy, and in practice, new actions are randomly selected on every state transition.
	\end{itemize}

	\begin{figure}[htbp]
	  \centering
	  \includegraphics[width=0.5\linewidth]{random_policy.png}
	  \caption{5 x 5 Maze with Random Policy}
	  \label{fig:random_policy}
	\end{figure}

    \item Assume the agent uses a Monte Carlo method to learn an optimal policy starting with the equi-random policy. It involves improving the random policy slowly using a greedy approach and stopping when the policy does not improve any more. Write the algorithm in terms of pseudocode and briefly explain with reference to lines in the pseudocode.


    \begin{algorithm}
    \caption{Optimize Policy with Every-Visit Monte Carlo}
    \begin{algorithmic}[1]
    \State Initialize policy to $\epsilon$-greedy policy \label{alg:greedy_policy}
    \State Initialize $V(s)$ to 0 for all $s$ in dynamics.keys()
    \State Set $V(\text{goal\_state})$ to 100
    \State Initialize returns to an empty list for all $s$ in dynamics.keys()
    \State Initialize policy\_changed to True
    \State Initialize num\_iterations to 0
    \State Initialize actions\_taken to an empty list
    \While{policy\_changed}
	\State Update $\epsilon$ with decay
	\State Generate an episode using current policy
	\State Get original policy output with compare\_policy
	\State Initialize goodness to 0
	\For{each step $(s, a, r)$ in episode, reversed}\label{alg:reversed_episode}
	    \State Update goodness with discount and reward
	    \State Append goodness to returns for state $s$
	    \State Update $V(s)$ with average of returns for $s$
	    \State Append action $a$ to actions\_taken
	\EndFor
	\State Get new policy output with compare\_policy \label{alg:action_compare}
	\If{policy unchanged and episode not truncated}
	    \State Set policy\_changed to False
	    \State Append reversed actions\_taken and num\_iterations to file
	\EndIf
	\State Clear actions\_taken
	\State Increment num\_iterations
    \EndWhile
    \State \Return value\_function
    \end{algorithmic}
      \label{alg:every_visit_mc}
    \end{algorithm}




    % \begin{algorithm}
    % \caption{Every Visit Monte Carlo Approach for Solving a 5x5 Maze}
    % \begin{algorithmic}[1] % Numbers each line
    % \Procedure{generateOptimizedPolicy}{$\gamma$}
    %     \State $policy \gets RandomPolicy()$
    %     \State $V(s) \gets 0$ for all $s \in S$
    %     \State $Returns(s) \gets []$ for all $s \in S$
    %     \State $PolicyChanged \gets \text{True}$
    %     \State $previousActions \gets []$
    %     \State $actionsTaken \gets []$
    %     \While{$PolicyChanged$}
    %         \State $Episode \gets GenerateEpisode(policy)$
    %         \State $G \gets 0$
    %         \For{$(s, a, r) \in \text{reversed}(Episode)$} \label{alg:reversed_episode}
    %     	\State $G \gets \gamma \times G + r$
    %     	\State $Returns(s).append(G)$
    %     	\State $V(s) \gets \text{Average}(Returns(s))$
    %     	\State $actionsTaken.append(a)$
    %         \EndFor
    %         \State $OldPolicy \gets policy$
    %         \State $policy \gets GreedyPolicy(V)$ \label{alg:greedy_policy}
    %         \If{$actionsTaken == previousActions$} \label{alg:action_compare}
    %     	\State $PolicyChanged \gets \text{False}$
    %         \EndIf
    %         \State $previousActions \gets actionsTaken$
    %     \EndWhile
    %     \State \textbf{return} $policy$
    % \EndProcedure
    % \end{algorithmic}
    %     \label{alg:every_visit_mc}
    % \end{algorithm}

	\begin{itemize}
	    \item The pseudocode in \ref{alg:every_visit_mc} is adapted from the every visit Monte Carlo approach \cite{SuttonBartoRLBook}.
	    \item Policies are compared based on actions taken wih $\epsilon=0$ (line \ref{alg:action_compare}).
	    \item The episode is unpacked in reverse to most efficiently calculate $G_t$ (line \ref{alg:reversed_episode}), using a dynamic programming approach to calculate the goodness of each episode.
	    \item An epsilon decay was added to encourage maximum exploration at the begining of training, slowly transitioning to more exploitation. Epsilon decays according to \ref{eq:eps_decay}.
	\end{itemize}

    \item Implement the algorithm using any programming language of choice. Run the algorithm several times. Does it produce the same policy every time? Show the policy (policies) learned. Explain any differences if any. Change any relevant parameters and rerun a few times. Comment on how changes in parameter values changes learning.
	\begin{itemize}
	    \item The generalized policy iteration (GPI) algorithm above was implemented in Python. Because the dynamics of the environment were implemented such that the state transitions were known to the agent, only the value function $V(s)$ was needed for policy improvement. It would also be straightforward to change this to use the action-value function $q(s,a)$ if the state transitions were unknown to the agent.
	    \item Running the algorithm 100,000 times with discount factor $\gamma=0.9$ yeilded two "optimal" policies $\pi_{*1}$ \ref{eq:pi1} (visualized in \ref{fig:pi1}) and $\pi_{*2}$ \ref{eq:pi2} (visualized in \ref{fig:pi2})

	    \begin{equation}
		\label{eq:eps_decay}
		\epsilon = e^{-0.1 \times \text{iteration}}
	    \end{equation}

	    \begin{gather}
		\pi_{*1} = \{D,D,D,R,R,R,R,D\} \\
		D = DOWN \\
		R = RIGHT
	    \label{eq:pi1}
	    \end{gather}

	    \begin{figure}[htbp]
	      \centering
	      \includegraphics[width=0.5\linewidth]{pi1.jpg}
		\caption{5 x 5 Maze with $\pi_{*1}$ Policy and State Value Function}
	      \label{fig:pi1}
	    \end{figure}

	    \begin{gather}
		\pi_{*2} = \{D,D,D,R,R,R,D,R\} \\
	    \label{eq:pi2}
	    \end{gather}

	    \begin{figure}[htbp]
	      \centering
	      \includegraphics[width=0.5\linewidth]{pi2.jpg}
		\caption{5 x 5 Maze with $\pi_{*2}$ Policy and State Value Function}
	      \label{fig:pi2}
	    \end{figure}

	    \item The table \ref{tab:gamma} shows data collected running the algorithm 10,000 times each for various $\gamma$ values.

	    \begin{table}[htbp]
	    \caption{Every Visit Policy Iterations}
	    \label{tab:gamma}
	    \centering
	    \begin{tabular}{|c|c|c|}
	    \hline
	    Gamma & Mean N. Episodes   & Std. Dev. \\ \hline
	    0.1   & 4.5091             & 1.8944    \\ \hline
	    0.2   & 4.5198             & 1.9355    \\ \hline
	    0.5   & 4.5451             & 1.9632    \\ \hline
	    0.9   & 5.4576             & 2.4856    \\ \hline
	    1.0   & 26.407             & 16.867    \\ \hline
	    \end{tabular}
	    \end{table}

	\end{itemize}
    \end{enumerate}




\bibliography{homework1}


\end{document}
