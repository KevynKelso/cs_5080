\def\year{2022}\relax
%File: formatting-instructions-latex-2022.tex
%release 2022.1
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai22}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{amsmath}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
% \usepackage{algorithm}
% \usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
%\nocopyright
%
% PDF Info Is REQUIRED.
% For /Title, write your title in Mixed Case.
% Don't use accents or commands. Retain the parentheses.
% For /Author, add all authors within the parentheses,
% separated by commas. No accents, special characters
% or commands are allowed.
% Keep the /TemplateVersion tag as is
\pdfinfo{
/Title Homework 1
/Author Kevyn Kelso
/TemplateVersion (2022.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai22.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Homework 1}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Kevyn Kelso
}
\affiliations{
    %Afiliations
    University of Colorado-Colorado Springs\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar, \textsuperscript{\rm 2}
    % J. Scott Penberthy, \textsuperscript{\rm 3}
    % George Ferguson,\textsuperscript{\rm 4}
    % Hans Guesgen, \textsuperscript{\rm 5}.
    % Note that the comma should be placed BEFORE the superscript for optimum readability
    1420 Austin Bluffs Pkwy\\
    Colorado Springs, CO 80919\\

    % email address must be in roman text type, not monospace or sans serif
    kkelso@uccs.edu
%
% See more examples next
}

\begin{document}

\maketitle

\begin{abstract}
This short paper studies reinforcement learning techniques to solving grid worlds of various complexity. The effectiveness of Every-Visit Monte Carlo Control methods applied to grids of (5x5, 10x10, 20x20, and 30x30) is evaluated. An $\epsilon$-greedy policy to balance exploration and exploitation, with an exponential decay is also applied. This research highlights the impact of grid size, episode truncation, policy comparison, and discount factor adjustments on the convergence to optimal policies.
\end{abstract}

\section{Introduction}
Exploring pathfinding strategies for grid environments is a popular reinforcement learning challenge. This paper presents an Every-Visit Monte Carlo method to find optimal policies within randomly generated mazes of different sizes. Various policies were applied ending at an $\epsilon$-greedy policy with epsilon decaying at an exponential rate. This paper focuses on evaluating this method across different maze sizes, examining the nuances of policy optimization, epsilon decay strategies, and how state-value functions can be applied to form an optimal policy when the environment's dynamics are known. Through a series of experiments, we aim to test how Monte Carlo methods perform in grid world environments with increasing complexity.


\section{Question 1}
	\begin{itemize}
	    \item For this project, the model given in \cite{Morales2020GrokkingDL} where the dynamics are represented as a nested dictionary in Python will be used. First, the outer dictionary keys will include all the states of the grid environment. Then, an inner dictionary will be the value for each state element containing a key for each action, and a value with a list of all transitions for the state-action pair.
	    \item The list will contain: the probability of that transition occurring given the state and action (in this case the probabilities are always 1 due to the non-stochastic nature of the problem), the next state (represented as an integer), the reward (all 0 except for the goal state), and a flag to tell whether the next state is terminal.
	    \item The barriers will be represented impicitly based on the dynamics dictionary available actions. The dynamics dictionary will be generated such that actions leading to barrier states are not included in available actions for that state. The same idea is applied to the edges, there will be no actions that lead off the grid.
	    \item The dynamics dictionary will be generated based on a given grid size and a list of obstacle coordinates.
	\end{itemize}

\section{Question 2}
	\begin{itemize}
	    \item This policy is shown in graphical form \ref{fig:random_policy} where arrows pointing left, right, down, and up indicate which action the agent will take in that state.
	    \item Note, this is a snapshot of the random policy, and in practice, new actions are randomly selected on every state transition.
	\end{itemize}

	\begin{figure}[htbp]
	  \centering
	  \includegraphics[width=0.5\linewidth]{random_policy.png}
	  \caption{5 x 5 Maze with Random Policy}
	  \label{fig:random_policy}
	\end{figure}

\section{Question 3}
    \begin{algorithm}
    \caption{Optimize Policy with Every-Visit Monte Carlo}
    \begin{algorithmic}[1]
    \State Initialize policy to $\epsilon$-greedy policy \label{alg:greedy_policy}
    \State Initialize $V(s)$ to 0 for all $s$ in dynamics.keys()
	\State Set $V(\text{goal\_state})$ to 100 \label{alg:high_value_goal}
    \State Initialize returns to an empty list for all $s$ in dynamics.keys()
    \State Initialize policy\_changed to True
    \State Initialize num\_iterations to 0
    \State Initialize actions\_taken to an empty list
    \State Initialize $\epsilon$ to 1.0
    \While{policy\_changed}
	\State Update $\epsilon$ with decay
	\State Generate an episode using current policy
	\State Get original policy output with compare\_policy
	\State Initialize goodness to 0
	\For{each step $(s, a, r)$ in episode, reversed}\label{alg:reversed_episode}
	    \State Update goodness with discount and reward
	    \State Append goodness to returns for state $s$
	    \State Update $V(s)$ with average of returns for $s$
	    \State Append action $a$ to actions\_taken
	\EndFor
	\State Get new policy output with compare\_policy \label{alg:action_compare}
	\If{policy unchanged and episode not truncated}
	    \State Set policy\_changed to False
	    \State Append reversed actions\_taken and num\_iterations to file
	\EndIf
	\State Clear actions\_taken
	\State Increment num\_iterations
    \EndWhile
    \State \Return value\_function
    \end{algorithmic}
      \label{alg:every_visit_mc}
    \end{algorithm}

	\begin{itemize}
	    \item The pseudocode in \ref{alg:every_visit_mc} is adapted from the every visit Monte Carlo approach \cite{SuttonBartoRLBook}.
	    \item Policies were originally compared based on actions taken with $\epsilon=0$ (line \ref{alg:action_compare}). In this case, the $\epsilon-greedy$ policy will choose the action leading to the higest valued state. This was later changed to be a root mean difference comparison of value functions. This is discussed in more detail below \ref{eq:value_fn_compare}.
	    \item The episode is unpacked in reverse to most efficiently calculate $G_t$ (line \ref{alg:reversed_episode}), using a dynamic programming approach to calculate the goodness of each episode.
	    \item An epsilon decay was added to encourage maximum exploration at the beginning of training, slowly transitioning to more exploitation. Epsilon begins at 1.0 decays according to \ref{eq:eps_decay}.
	    \item The value of the goal state is manually set to a high value in \ref{alg:high_value_goal} due to the value function $V(s)$ always assigning 0 to terminal states \cite{Morales2020GrokkingDL}.
	    \item The state-value function $V(s)$ was used instead of the action-value function $Q(s,a)$ because the dynamics dictionary is available to the agent. To compute the $(argmax(V(s))$, each action is considered, and the policy chooses the action leading the agent to the maximum valued next state. In hindsight, using $Q(s,a)$ would likely have been simpler.
	\end{itemize}

\section{Question 4}
	\begin{itemize}
	    \item The generalized policy iteration (GPI) algorithm above was implemented in Python. Because the dynamics of the environment were implemented such that the state transitions were known to the agent, only the value function $V(s)$ was needed for policy improvement.
	    \item Running the algorithm 100,000 times with discount factor $\gamma=0.9$ yeilded two "optimal" policies $\pi_{*1}$ \ref{eq:pi1} (visualized in \ref{fig:pi1}) and $\pi_{*2}$ \ref{eq:pi2} (visualized in \ref{fig:pi2})

	    \begin{equation}
		\label{eq:eps_decay}
		\epsilon = e^{-0.1 \times \text{iteration}}
	    \end{equation}

	    \begin{gather}
		\pi_{*1} = \{D,D,D,R,R,R,R,D\} \\
		D = DOWN \\
		R = RIGHT
	    \label{eq:pi1}
	    \end{gather}

	    \begin{figure}[htbp]
	      \centering
	      \includegraphics[width=0.5\linewidth]{pi1.jpg}
		\caption{5 x 5 Maze with $\pi_{*1}$ Policy and State Value Function}
	      \label{fig:pi1}
	    \end{figure}

	    \begin{gather}
		\pi_{*2} = \{D,D,D,R,R,R,D,R\}
	    \label{eq:pi2}
	    \end{gather}

	    \begin{figure}[htbp]
	      \centering
	      \includegraphics[width=0.5\linewidth]{pi2.jpg}
		\caption{5 x 5 Maze with $\pi_{*2}$ Policy and State Value Function}
	      \label{fig:pi2}
	    \end{figure}

	    \item The table \ref{tab:gamma} shows data collected running the algorithm 10,000 times each for various $\gamma$ values.

	    \begin{table}[htbp]
	    \caption{Every Visit Policy Iterations}
	    \label{tab:gamma}
	    \centering
	    \begin{tabular}{|c|c|c|}
	    \hline
	    Gamma & Mean N. Episodes   & Std. Dev. \\ \hline
	    0.1   & 4.5091             & 1.8944    \\ \hline
	    0.2   & 4.5198             & 1.9355    \\ \hline
	    0.5   & 4.5451             & 1.9632    \\ \hline
	    0.9   & 5.4576             & 2.4856    \\ \hline
	    1.0   & 26.407             & 16.867    \\ \hline
	    \end{tabular}
	    \end{table}

	\end{itemize}

\section{Question 5}
	\begin{itemize}
	    \item A Depth First Search approach to maze generation was employed to create larger grid structures \cite{CopeMazeGenerationDFS}.
	    \item The 10x10 grid was done in a similar way to the 5x5 except the epsilon decay was changed to only decay once all states were visited at least once, and an episode truncation limit was introduced. The episode truncation limit was set to 400 steps for the 10x10 maze. The optimal solution found by every visit MC is shown in \ref{fig:10x10_solution}.
	    \item The path in red indicates the last path the agent took before convergence was declared.

	    \begin{figure}[htbp]
	      \centering
	      \includegraphics[width=0.5\linewidth]{10x10.jpg}
		\caption{10 x 10 Maze with Optimal Policy and State Value Function}
	      \label{fig:10x10_solution}
	    \end{figure}

	    \item For the 20x20 grid, $\gamma < 0.9$ resulted in $G_t$ calculations that exceeded the lower limit of 64-bit precision in Python. Thus, $\gamma$ was increased to $0.99$ for the remainder of the experiments.

	    \begin{figure}[htbp]
	      \centering
	      \includegraphics[width=0.5\linewidth]{20x20.jpg}
		\caption{20 x 20 Maze}
	      \label{fig:20x20_solution}
	    \end{figure}

	    \item The 30x30 grid required the implementation of episode truncation to complete it in a reasonable amount of time. An adaptable truncation limit was introduced starting at a 1000-step limit and increasing if an episode was truncated. Truncated episodes were not evaluated for policy evaluation since the goodness of the episode would always be 0 (the agent never made it to the goal state).
	    \item A major challenge of the more complex mazes was how to determine if one policy was better than another. This was originally done by looking at the goodness of a training episode, however, this was changed to be a comparison of value functions shown in \ref{eq:value_fn_compare}.

	    \begin{equation}
		\text{diff} = \sqrt{\left| \frac{1}{N} \sum_{i=1}^{N} (oldValues_i - newValues_i) \right| }
		\label{eq:value_fn_compare}
	    \end{equation}

	    Here, \( N \) is the total number of elements in each of the old values and new values arrays. The outer absolute value ensures that the quantity under the square root is non-negative, as required by the square root function.

	    \begin{figure}[htbp]
	      \centering
	      \includegraphics[width=0.3\linewidth]{30x30.jpg}
		\caption{30 x 30 Maze}
	      \label{fig:30x30_solution}
	    \end{figure}

	    \item To test the employed algorithm further, 40x40 grids were also explored. 

	    \begin{figure}[htbp]
	      \centering
	      \includegraphics[width=0.3\linewidth]{40x40.jpg}
		\caption{40 x 40 Maze}
	      \label{fig:40x40_solution}
	    \end{figure}

	    % \begin{figure}[htbp]
	    %   \centering
	    %   \includegraphics[width=0.5\linewidth]{100x100.jpg}
	    %     \caption{100 x 100 Maze}
	    %   \label{fig:100x100_solution}
	    % \end{figure}


	\item Number of episodes required to stabilize are summarized in table \ref{tab:num_episodes}. Unlike the 5x5 maze, the mean and standard deviation were not computed in the interest of time.

	    \begin{table}[htbp]
	    \caption{Monte Carlo Mazes}
	    \label{tab:num_episodes}
	    \centering
	    \begin{tabular}{|c|c|c|c|}
	    \hline
		Maze    &  N. Episodes & N. Episode Steps & Optimal G       \\ \hline
		10x10   & 88        & 18                  & 0.983   \\ \hline
		20x20   & 109       & 52                  & 0.950   \\ \hline
		30x30   & 116       & 150                 & 0.862   \\ \hline
		% 40x40   & 208    \\ \hline
	    \end{tabular}
	    \end{table}

	    \begin{figure}[htbp]
	      \centering
	      \includegraphics[width=1.3\linewidth]{40x40_metrics.png}
		\caption{30 x 30 Maze Training Data}
	      \label{fig:metrics}
	    \end{figure}
	     
	\end{itemize}

\section{Conclusion}
This research demonstrates the Every-Visit Monte Carlo approach to solving maze navigation in several complexities. 5x5, 10x10, 20x20, and 30x30 mazes reveal the need for an $\epsilon$-greedy policy, epsilon decay, and the setting of episode truncation limits to enhancing policy optimization efficiency. The utilization of the state-value function $V(s)$ instead of the action-value function $Q(s,a)$, given the known dynamics, proved to work. However, if it was to be done again, the action-value function would have made the problem simpler. The convergence to "optimal" policies, at least for the complexities explored highlights the effectiveness of Monte Carlo Control for grid worlds. Future work may explore the more sophisticated algorithms.

\bibliography{homework1}


\end{document}
