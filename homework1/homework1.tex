\def\year{2022}\relax
%File: formatting-instructions-latex-2022.tex
%release 2022.1
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai22}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{amsmath}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
% \usepackage{algorithm}
% \usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
%\nocopyright
%
% PDF Info Is REQUIRED.
% For /Title, write your title in Mixed Case.
% Don't use accents or commands. Retain the parentheses.
% For /Author, add all authors within the parentheses,
% separated by commas. No accents, special characters
% or commands are allowed.
% Keep the /TemplateVersion tag as is
\pdfinfo{
/Title Homework 1
/Author Kevyn Kelso
/TemplateVersion (2022.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai22.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Homework 1}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Kevyn Kelso
}
\affiliations{
    %Afiliations
    University of Colorado-Colorado Springs\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar, \textsuperscript{\rm 2}
    % J. Scott Penberthy, \textsuperscript{\rm 3}
    % George Ferguson,\textsuperscript{\rm 4}
    % Hans Guesgen, \textsuperscript{\rm 5}.
    % Note that the comma should be placed BEFORE the superscript for optimum readability
    1420 Austin Bluffs Pkwy\\
    Colorado Springs, CO 80919\\

    % email address must be in roman text type, not monospace or sans serif
    kkelso@uccs.edu
%
% See more examples next
}

\begin{document}

\maketitle

\section{Questions}
\begin{enumerate}
    \item How would you represent the various components that are required to program this reinforcement learning agent? Discuss each component and how you implement it. Explain how you model the barriers, and how you make sure the agent does not fall off the edges.
	\begin{itemize}
	    \item I plan to follow the model given in \cite{Morales2020GrokkingDL} where the dynamics are represented as a nested dictionary in Python. First, the outer dictionary keys will include all the states of the grid environment. Then, an inner dictionary will be the value for each state element containing a key for each action, and a value with a list of all transitions for the state-action pair.
	    \item The list will contain: the probability of that transition occurring given the state and action (in this case the probabilities are always 1 due to the non-stochastic nature of the problem), the next state (represented as an integer), the reward (all 0 except for the goal state), and a flag to tell whether the next state is terminal.
	    \item The barriers will be a property of the dynamics dictionary. The actions that would lead the agent to a barrier state will not be included in the dynamics, making it impossible to transition to those barrier states.
	    \item The same idea is applied to the edges.
	    \item Ideally, these dynamics will be automatically generated based on a given grid environment.
	\end{itemize}
    \item You would initialize with a random policy where every action that is possible in a state is equally probable. What does this policy look like?
	\begin{itemize}
	    \item This policy is best viewed in graphical form \ref{fig:random_policy} where arrows pointing left, right, down, and up indicate which action the agent will take in that state.
	    \item Note, this is a snapshot of the random policy, and in practice, new actions are randomly selected on every state transition.
	\end{itemize}

	\begin{figure}[htbp]
	  \centering
	  \includegraphics[width=0.5\linewidth]{random_policy.png}
	  \caption{5 x 5 Maze with Random Policy}
	  \label{fig:random_policy}
	\end{figure}

    \item Assume the agent uses a Monte Carlo method to learn an optimal policy starting with the equi-random policy. It involves improving the random policy slowly using a greedy approach and stopping when the policy does not improve anymore. Write the algorithm in terms of pseudocode and briefly explain with reference to lines in the pseudocode.


    \begin{algorithm}
    \caption{Optimize Policy with Every-Visit Monte Carlo}
    \begin{algorithmic}[1]
    \State Initialize policy to $\epsilon$-greedy policy \label{alg:greedy_policy}
    \State Initialize $V(s)$ to 0 for all $s$ in dynamics.keys()
	\State Set $V(\text{goal\_state})$ to 100 \label{alg:high_value_goal}
    \State Initialize returns to an empty list for all $s$ in dynamics.keys()
    \State Initialize policy\_changed to True
    \State Initialize num\_iterations to 0
    \State Initialize actions\_taken to an empty list
    \While{policy\_changed}
	\State Update $\epsilon$ with decay
	\State Generate an episode using current policy
	\State Get original policy output with compare\_policy
	\State Initialize goodness to 0
	\For{each step $(s, a, r)$ in episode, reversed}\label{alg:reversed_episode}
	    \State Update goodness with discount and reward
	    \State Append goodness to returns for state $s$
	    \State Update $V(s)$ with average of returns for $s$
	    \State Append action $a$ to actions\_taken
	\EndFor
	\State Get new policy output with compare\_policy \label{alg:action_compare}
	\If{policy unchanged and episode not truncated}
	    \State Set policy\_changed to False
	    \State Append reversed actions\_taken and num\_iterations to file
	\EndIf
	\State Clear actions\_taken
	\State Increment num\_iterations
    \EndWhile
    \State \Return value\_function
    \end{algorithmic}
      \label{alg:every_visit_mc}
    \end{algorithm}

	\begin{itemize}
	    \item The pseudocode in \ref{alg:every_visit_mc} is adapted from the every visit Monte Carlo approach \cite{SuttonBartoRLBook}.
	    \item Policies are compared based on actions taken wih $\epsilon=0$ (line \ref{alg:action_compare}).
	    \item The episode is unpacked in reverse to most efficiently calculate $G_t$ (line \ref{alg:reversed_episode}), using a dynamic programming approach to calculate the goodness of each episode.
	    \item An epsilon decay was added to encourage maximum exploration at the beginning of training, slowly transitioning to more exploitation. Epsilon decays according to \ref{eq:eps_decay}.
	    \item The value of the goal state is manually set to a high value in \ref{alg:high_value_goal} due to the value function $V(s)$ always assigning 0 to terminal states \cite{Morales2020GrokkingDL}.
	\end{itemize}

    \item Implement the algorithm using any programming language of choice. Run the algorithm several times. Does it produce the same policy every time? Show the policy (policies) learned. Explain any differences if any. Change any relevant parameters and rerun a few times. Comment on how changes in parameter values change learning.
	\begin{itemize}
	    \item The generalized policy iteration (GPI) algorithm above was implemented in Python. Because the dynamics of the environment were implemented such that the state transitions were known to the agent, only the value function $V(s)$ was needed for policy improvement. It would also be straightforward to change this to use the action-value function $q(s, a)$ if the state transitions were unknown to the agent.
	    \item Running the algorithm 100,000 times with discount factor $\gamma=0.9$ yeilded two "optimal" policies $\pi_{*1}$ \ref{eq:pi1} (visualized in \ref{fig:pi1}) and $\pi_{*2}$ \ref{eq:pi2} (visualized in \ref{fig:pi2})

	    \begin{equation}
		\label{eq:eps_decay}
		\epsilon = e^{-0.1 \times \text{iteration}}
	    \end{equation}

	    \begin{gather}
		\pi_{*1} = \{D,D,D,R,R,R,R,D\} \\
		D = DOWN \\
		R = RIGHT
	    \label{eq:pi1}
	    \end{gather}

	    \begin{figure}[htbp]
	      \centering
	      \includegraphics[width=0.5\linewidth]{pi1.jpg}
		\caption{5 x 5 Maze with $\pi_{*1}$ Policy and State Value Function}
	      \label{fig:pi1}
	    \end{figure}

	    \begin{gather}
		\pi_{*2} = \{D,D,D,R,R,R,D,R\}
	    \label{eq:pi2}
	    \end{gather}

	    \begin{figure}[htbp]
	      \centering
	      \includegraphics[width=0.5\linewidth]{pi2.jpg}
		\caption{5 x 5 Maze with $\pi_{*2}$ Policy and State Value Function}
	      \label{fig:pi2}
	    \end{figure}

	    \item The table \ref{tab:gamma} shows data collected running the algorithm 10,000 times each for various $\gamma$ values.

	    \begin{table}[htbp]
	    \caption{Every Visit Policy Iterations}
	    \label{tab:gamma}
	    \centering
	    \begin{tabular}{|c|c|c|}
	    \hline
	    Gamma & Mean N. Episodes   & Std. Dev. \\ \hline
	    0.1   & 4.5091             & 1.8944    \\ \hline
	    0.2   & 4.5198             & 1.9355    \\ \hline
	    0.5   & 4.5451             & 1.9632    \\ \hline
	    0.9   & 5.4576             & 2.4856    \\ \hline
	    1.0   & 26.407             & 16.867    \\ \hline
	    \end{tabular}
	    \end{table}
	\end{itemize}

    \item Create larger grid structures with randomly placed barriers, with start and goal states located like the original configuration. Repeat the experiments with grid sizes of 10 × 10, 20 × 20, and 30 × 30. Make observations regarding the stability of solutions, the number of episodes needed to stabilize, solution quality, computational complexity, and anything else you find interesting.
	\begin{itemize}
	    \item A Depth First Search approach to maze generation was employed to create larger grid structures \cite{CopeMazeGenerationDFS}.
	    \item The 10x10 grid was done in a similar way to the 5x5 except the epsilon decay was changed to only decay once all states were visited at least once, and an episode truncation limit was introduced. The episode truncation limit was set to 400 steps for the 10x10 maze. The optimal solution found by every visit MC is shown in \ref{fig:10x10_solution}.
	    \item The path in red indicates the last path the agent took before convergence was declared.

	    \begin{figure}[htbp]
	      \centering
	      \includegraphics[width=0.5\linewidth]{10x10.jpg}
		\caption{10 x 10 Maze with Optimal Policy and State Value Function}
	      \label{fig:10x10_solution}
	    \end{figure}

	    \item For the 20x20 grid, $\gamma < 0.9$ resulted in $G_t$ calculations that exceeded the lower limit of 64-bit precision in Python. Thus, $\gamma$ was increased to $0.99$ for the remainder of the experiments.

	    \begin{figure}[htbp]
	      \centering
	      \includegraphics[width=0.5\linewidth]{20x20.jpg}
		\caption{20 x 20 Maze}
	      \label{fig:20x20_solution}
	    \end{figure}

	    \item The 30x30 grid required the implementation of episode truncation to complete it in a reasonable amount of time. An adaptable truncation limit was introduced starting at a 1000-step limit and increasing if an episode was truncated. Truncated episodes were not evaluated for policy evaluation since the goodness of the episode would always be 0 (the agent never made it to the goal state).

	    \begin{figure}[htbp]
	      \centering
	      \includegraphics[width=0.5\linewidth]{30x30.jpg}
		\caption{30 x 30 Maze}
	      \label{fig:30x30_solution}
	    \end{figure}

	    \item To test the employed algorithm further, 40x40 grids were also explored. 

	    \begin{figure}[htbp]
	      \centering
	      \includegraphics[width=0.5\linewidth]{40x40.jpg}
		\caption{40 x 40 Maze}
	      \label{fig:40x40_solution}
	    \end{figure}

	    % \begin{figure}[htbp]
	    %   \centering
	    %   \includegraphics[width=0.5\linewidth]{100x100.jpg}
	    %     \caption{100 x 100 Maze}
	    %   \label{fig:100x100_solution}
	    % \end{figure}


	\item Number of episodes required to stabilize are summarized in table \ref{tab:num_episodes}. Unlike the 5x5 maze, the mean and standard deviation were not computed in the interest of time.

	    \begin{table}[htbp]
	    \caption{Monte Carlo Mazes}
	    \label{tab:num_episodes}
	    \centering
	    \begin{tabular}{|c|c|c|c|}
	    \hline
		Maze    &  N. Episodes & N. Episode Steps & Optimal G       \\ \hline
		10x10   & 88        & 18                  & 0.983   \\ \hline
		20x20   & 109       & 52                  & 0.950   \\ \hline
		30x30   & 116       & 150                 & 0.862   \\ \hline
		% 40x40   & 208    \\ \hline
	    \end{tabular}
	    \end{table}
	\end{itemize}

    \end{enumerate}




\bibliography{homework1}


\end{document}
